{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinvegda/Desktop/code/dspy-projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kevinvegda/Desktop/code/dspy-projects/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.1 at grpc_health/v1/health.proto. Please avoid checked-in Protobuf gencode that can be obsolete.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "  \n",
    "WCS_API_KEY = os.getenv(\"WCS_API_KEY\")\n",
    "WEAVIATE_CLUSTER_URL= os.getenv(\"WEAVIATE_CLUSTER_URL\")\n",
    "  \n",
    "# Connect to a WCS instance\n",
    "weaviate_client = weaviate.connect_to_wcs(\n",
    "    cluster_url=WEAVIATE_CLUSTER_URL,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(WCS_API_KEY),\n",
    "    headers = {\n",
    "        'X-Openai-Api-Key': os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    "    )\n",
    "\n",
    "llm = dspy.OpenAI(model = \"gpt-4o\")\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "\n",
    "dspy.settings.configure(lm = llm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why would I use Weaviate as my vector database?',\n",
       " 'What is the difference between Weaviate and for example Elasticsearch?',\n",
       " 'Do you offer Weaviate as a managed service?',\n",
       " 'How should I configure the size of my instance?',\n",
       " 'Do I need to know about Docker (Compose) to use Weaviate?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "f = open(\"faq.md\")\n",
    "markdown_content = f.read()\n",
    "\n",
    "def parse_question(markdown_content):\n",
    "    question_pattern = r'#### Q: (.+?)\\n'\n",
    "    questions = re.findall(question_pattern, markdown_content, re.DOTALL)\n",
    "    return questions\n",
    "\n",
    "questions = parse_question(markdown_content)\n",
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset =  questions[:20]\n",
    "devset = questions[20:30]\n",
    "testset = questions[30:]\n",
    "\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in trainset]\n",
    "devset = [dspy.Example(question=question).with_inputs(\"question\") for question in devset]\n",
    "testset = [dspy.Example(question=question).with_inputs(\"question\") for question in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM = dspy.OpenAI(model = 'gpt-4o', max_tokens = 1000, model_type='chat')\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of an answer to a question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc = \"The context for answering the question\")\n",
    "    assessment_criterion = dspy.InputField(desc = \"The evaluation criterion\")\n",
    "    assessed_answer = dspy.InputField(desc = \"The answer to the question\")\n",
    "    assessment_answer = dspy.OutputField(desc = \"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "def llm_metric(gold, pred, trace = None):\n",
    "    predicted_answer = pred.answer\n",
    "    question = gold.question\n",
    "\n",
    "    print(f\"Test question: {question}\")\n",
    "    print(f\"Predicted answer: {predicted_answer}\")\n",
    "\n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\"\n",
    "    overall = f\"Please rate how well this answer addresses the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "\n",
    "    with dspy.context(lm = metricLM):\n",
    "        context = dspy.Retrieve(k = 5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context = 'N/A', assessment_criterion = detail, assessed_answer = predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context = context, assessment_criterion = faithful, assessed_answer = predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context = context, assessment_criterion = overall, assessed_answer = predicted_answer)\n",
    "\n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "\n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer) * 2 + float(overall.assessment_answer)\n",
    "\n",
    "    return total / 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What do cross encoders do?\n",
      "Predicted answer: They re-rank documents.\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question = \"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer = \"They re-rank documents.\")\n",
    "\n",
    "llm_metric(test_example, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What do cross encoders do?\n",
      "Predicted answer: They index data.\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They index data.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\n",
      "[4] «![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
      "[5] «To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\n",
      "\n",
      "We can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.»\n",
      "\n",
      "Assessment Criterion: Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to 1\n",
      "\n",
      "Assessment Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\n",
      "[4] «![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
      "[5] «To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\n",
      "\n",
      "We can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.»\n",
      "\n",
      "Assessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Assessment Answer: 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\n",
      "[4] «![Cross-Encoder](./img/cross-encoder.png)\n",
      "\n",
      "*Figure 3 - Representation of a Cross-Encoder model*\n",
      "\n",
      "\n",
      "If a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\n",
      "\n",
      "We can combine the two methods to benefit from the strong points of both models! I'd like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\n",
      "[5] «To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\n",
      "\n",
      "We can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.»\n",
      "\n",
      "Assessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\n",
      "\n",
      "Assessed Answer: They index data.\n",
      "\n",
      "Reasoning: Let's think step by step in order to Assessment Answer: 1\n",
      "\n",
      "Assessment Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\\n[4] «![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\\n[5] «To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\\n\\nWe can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.»\\n\\nAssessment Criterion: Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\\n\\nAssessed Answer: They index data.\\n\\nReasoning: Let\\'s think step by step in order to 1\\n\\nAssessment Answer:\\x1b[32m 1\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\\n[4] «![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\\n[5] «To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\\n\\nWe can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.»\\n\\nAssessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\\n\\nAssessed Answer: They index data.\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m Assessment Answer: 1\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let\\'s think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\\n[4] «![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.»\\n[5] «To get the most accurate results, we train our models on datasets representative of their specialized use case. Pretrained Sentence Transformers Cross-Encoder models are [available on HuggingFace](https://huggingface.co/Cross-Encoder). There you can find many different models, for example a model trained on MS Marco, which you could use in a search application with general natural language data (the model is trained on Bing search queries). If you have a search task for a dataset that is *out-of-domain*, you should train or fine-tune a model, see [here](https://www.sbert.net/examples/training/cross-encoder/README.html) for examples. ## Conclusion\\n\\nWe can combine the fast Bi-Encoders and accurate Cross-Encoders in a search pipeline to improve the search experience.»\\n\\nAssessment Criterion: Please rate how well this answer addresses the question, `What do cross encoders do?` based on the context. `They index data.`\\n\\nAssessed Answer: They index data.\\n\\nReasoning: Let\\'s think step by step in order to Assessment Answer: 1\\n\\nAssessment Answer:\\x1b[32m 1\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricLM.inspect_history(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc = \"May contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k = num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context = context, question = question)\n",
    "        return dspy.Prediction(answer = prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "Answer:\u001b[32m Context: Cross Encoders are a type of neural network architecture used in natural language processing tasks. They work by taking two input sequences (such as a pair of sentences) and processing them together to produce a single output, typically a similarity score or a classification label. This approach allows the model to consider the interactions between the two sequences in a more detailed manner compared to other architectures like Bi-Encoders, which process each sequence independently before combining their representations.\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "Answer: Cross Encoders are a type of neural network architecture used in natural language processing tasks that take two input sequences and process them together to produce a single output, allowing the model to consider the interactions between the sequences in detail.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\nQuestion: ${question}\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: What are Cross Encoders?\\nAnswer:\\x1b[32m Context: Cross Encoders are a type of neural network architecture used in natural language processing tasks. They work by taking two input sequences (such as a pair of sentences) and processing them together to produce a single output, typically a similarity score or a classification label. This approach allows the model to consider the interactions between the two sequences in a more detailed manner compared to other architectures like Bi-Encoders, which process each sequence independently before combining their representations.\\n\\nQuestion: What are Cross Encoders?\\nAnswer: Cross Encoders are a type of neural network architecture used in natural language processing tasks that take two input sequences and process them together to produce a single output, allowing the model to consider the interactions between the sequences in detail.\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.Predict(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Context: Cross Encoders are a type of model used in natural language processing tasks, particularly in tasks involving sentence pair classification. They work by taking two sentences as input and processing them together through a transformer model to produce a single output, which is typically a score indicating the relationship between the two sentences (e.g., similarity, entailment, etc.).\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to understand the role and function of Cross Encoders in natural language processing tasks. Cross Encoders take two sentences as input and process them together through a transformer model. The output is a single score that indicates the relationship between the two sentences.\n",
      "\n",
      "Answer: Cross Encoders are models\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nAnswer questions based on the context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: What are Cross Encoders?\\n\\nReasoning: Let's think step by step in order to\\x1b[32m Context: Cross Encoders are a type of model used in natural language processing tasks, particularly in tasks involving sentence pair classification. They work by taking two sentences as input and processing them together through a transformer model to produce a single output, which is typically a score indicating the relationship between the two sentences (e.g., similarity, entailment, etc.).\\n\\nQuestion: What are Cross Encoders?\\n\\nReasoning: Let's think step by step in order to produce the answer. We need to understand the role and function of Cross Encoders in natural language processing tasks. Cross Encoders take two sentences as input and process them together through a transformer model. The output is a single score that indicates the relationship between the two sentences.\\n\\nAnswer: Cross Encoders are models\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.ChainOfThought(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "You will be given `context`, `question` and you will respond with `answer`.\n",
      "\n",
      "To do this, you will interleave Thought, Action, and Observation steps.\n",
      "\n",
      "Thought can reason about the current situation, and Action can be the following types:\n",
      "\n",
      "(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n",
      "(2) Finish[answer], which returns the final `answer` and finishes the task\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Thought 1: next steps to take based on last observation\n",
      "\n",
      "Action 1: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\n",
      "\n",
      "Observation 1: observations based on action\n",
      "\n",
      "Thought 2: next steps to take based on last observation\n",
      "\n",
      "Action 2: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\n",
      "\n",
      "Observation 2: observations based on action\n",
      "\n",
      "Thought 3: next steps to take based on last observation\n",
      "\n",
      "Action 3: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\n",
      "\n",
      "Observation 3: observations based on action\n",
      "\n",
      "Thought 4: next steps to take based on last observation\n",
      "\n",
      "Action 4: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\n",
      "\n",
      "Observation 4: observations based on action\n",
      "\n",
      "Thought 5: next steps to take based on last observation\n",
      "\n",
      "Action 5: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "Thought 1: I need to find information about what cross encoders are.\n",
      "\n",
      "Action 1: Search[\"What are cross encoders?\"]\n",
      "\n",
      "Question: What are cross encoders?\n",
      "\n",
      "Thought 1: \n",
      "\n",
      "Action 1: Thought 1: I need to search for information on what cross encoders are to provide an accurate answer.\n",
      "\n",
      "Observation 1: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 2: Thought 1: I need to search for information on what cross encoders are to provide an accurate answer. Action 1: Search[\"What are cross encoders?\"] Observation 1: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models. Thought 2: I now have the information needed\n",
      "\n",
      "Action 2: Context:\n",
      "\n",
      "Observation 2: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 3: Context: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models. Question: What are cross encoders? Thought 1: I have the necessary information about cross encoders from the context provided. Action 1: Finish[Cross encoders are a type of neural network architecture used primarily in natural language\n",
      "\n",
      "Action 3: Context: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence\n",
      "\n",
      "Observation 3: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 4: Context: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models. Question: What are cross encoders? Thought 1: I have the necessary information about cross encoders from the context provided. Action 1: Finish[Cross encoders are a type of neural network architecture used primarily in natural language\n",
      "\n",
      "Action 4: Observation 4: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Observation 4: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 5: Thought 1: I need to provide a clear and concise answer based on the context provided. Action 1: Finish[Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models.]\n",
      "\n",
      "Action 5:\u001b[32m Observation 5: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 6: I need to provide a clear and concise answer based on the context provided. I will ensure the formatting is correct this time.\n",
      "\n",
      "Action 6: Finish[Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context\\n\\nYou will be given `context`, `question` and you will respond with `answer`.\\n\\nTo do this, you will interleave Thought, Action, and Observation steps.\\n\\nThought can reason about the current situation, and Action can be the following types:\\n\\n(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\\n(2) Finish[answer], which returns the final `answer` and finishes the task\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\n\\nQuestion: ${question}\\n\\nThought 1: next steps to take based on last observation\\n\\nAction 1: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\\n\\nObservation 1: observations based on action\\n\\nThought 2: next steps to take based on last observation\\n\\nAction 2: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\\n\\nObservation 2: observations based on action\\n\\nThought 3: next steps to take based on last observation\\n\\nAction 3: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\\n\\nObservation 3: observations based on action\\n\\nThought 4: next steps to take based on last observation\\n\\nAction 4: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\\n\\nObservation 4: observations based on action\\n\\nThought 5: next steps to take based on last observation\\n\\nAction 5: always either Search[query] or, when done, Finish[<answer>], where <answer> is the answer to the question itself.\\n\\n---\\n\\nContext:\\nThought 1: I need to find information about what cross encoders are.\\n\\nAction 1: Search[\"What are cross encoders?\"]\\n\\nQuestion: What are cross encoders?\\n\\nThought 1: \\n\\nAction 1: Thought 1: I need to search for information on what cross encoders are to provide an accurate answer.\\n\\nObservation 1: Failed to parse action. Bad formatting or incorrect action name.\\n\\nThought 2: Thought 1: I need to search for information on what cross encoders are to provide an accurate answer. Action 1: Search[\"What are cross encoders?\"] Observation 1: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models. Thought 2: I now have the information needed\\n\\nAction 2: Context:\\n\\nObservation 2: Failed to parse action. Bad formatting or incorrect action name.\\n\\nThought 3: Context: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models. Question: What are cross encoders? Thought 1: I have the necessary information about cross encoders from the context provided. Action 1: Finish[Cross encoders are a type of neural network architecture used primarily in natural language\\n\\nAction 3: Context: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence\\n\\nObservation 3: Failed to parse action. Bad formatting or incorrect action name.\\n\\nThought 4: Context: Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models. Question: What are cross encoders? Thought 1: I have the necessary information about cross encoders from the context provided. Action 1: Finish[Cross encoders are a type of neural network architecture used primarily in natural language\\n\\nAction 4: Observation 4: Failed to parse action. Bad formatting or incorrect action name.\\n\\nObservation 4: Failed to parse action. Bad formatting or incorrect action name.\\n\\nThought 5: Thought 1: I need to provide a clear and concise answer based on the context provided. Action 1: Finish[Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input sequences (e.g., a pair of sentences) and processing them together to produce a single output, which can be used for tasks such as sentence similarity, question answering, or text classification. Unlike bi-encoders, which encode each input sequence independently, cross encoders consider the interaction between the input sequences during the encoding process, often leading to more accurate but computationally expensive models.]\\n\\nAction 5:\\x1b[32m Observation 5: Failed to parse action. Bad formatting or incorrect action name.\\n\\nThought 6: I need to provide a clear and concise answer based on the context provided. I will ensure the formatting is correct this time.\\n\\nAction 6: Finish[Cross encoders are a type of neural network architecture used primarily in natural language processing tasks. They work by taking two input\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy.ReAct(GenerateAnswer, tools=[dspy.settings.rm])(question=\"What are cross encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-rankers in search engines are models or algorithms used to re-order search results to improve their relevance to the user's query. They take into account various features and data to provide a more accurate ranking. For example, Cross Encoders use a `(query, document)` pair to output a high precision relevance score, Metadata Rankers use symbolic features like user and document attributes to predict relevance\n"
     ]
    }
   ],
   "source": [
    "print(uncompiled_rag(\"What are re-rankers in search engines?\").answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\n",
      "[2] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\n",
      "[3] «LLM Re-Rankers stand to transform Cross Encoders by taking in more than one document for this high precision relevance calculation. This enables the transformer LLM to apply attention over the entire list of potential results. This is also heavily related to the concept of AutoCut with LLMs, this refers to giving the Language Model k search results and prompting it to determine how many of the k search results are relevant enough to either show to a human user, or pass along the next step of an LLM computation chain. Returning to the topic of XGBoost symbolic re-ranking, LLM re-rankers are also quite well-positioned to achieve innovations in symbolic re-ranking as well. For example, we can prompt the language model like this:\n",
      "\n",
      "```\n",
      "Please rank the following search results according to their relevance with the query: {query}.»\n",
      "\n",
      "Question: What are re-rankers in search engines?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Reasoning: Let's think step by step in order to produce the answer. We need to understand the concept of re-rankers in search engines based on the provided context. The context mentions different types of re-rankers, such as Cross Encoders, Metadata Rankers, and LLM Re-Rankers, and explains their functionalities and advantages.\n",
      "\n",
      "Answer: Re-rankers in search engines are models or algorithms used to re-order search results to improve their relevance to the user's query. They take into account various features and data to provide a more accurate ranking. For example, Cross Encoders use a `(query, document)` pair to output a high precision relevance score, Metadata Rankers use symbolic features like user and document attributes to predict relevance\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nAnswer questions based on the context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nContext:\\n[1] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\\n[2] «As described in our [previous article](https://weaviate.io/blog/ranking-models-for-better-search), re-ranking models are new to the scene of zero-shot generalization. The story of re-rankers has mostly been tabular user features combined with tabular product or item features, fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt. Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score. This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.»\\n[3] «LLM Re-Rankers stand to transform Cross Encoders by taking in more than one document for this high precision relevance calculation. This enables the transformer LLM to apply attention over the entire list of potential results. This is also heavily related to the concept of AutoCut with LLMs, this refers to giving the Language Model k search results and prompting it to determine how many of the k search results are relevant enough to either show to a human user, or pass along the next step of an LLM computation chain. Returning to the topic of XGBoost symbolic re-ranking, LLM re-rankers are also quite well-positioned to achieve innovations in symbolic re-ranking as well. For example, we can prompt the language model like this:\\n\\n```\\nPlease rank the following search results according to their relevance with the query: {query}.»\\n\\nQuestion: What are re-rankers in search engines?\\n\\nReasoning: Let's think step by step in order to\\x1b[32m Reasoning: Let's think step by step in order to produce the answer. We need to understand the concept of re-rankers in search engines based on the provided context. The context mentions different types of re-rankers, such as Cross Encoders, Metadata Rankers, and LLM Re-Rankers, and explains their functionalities and advantages.\\n\\nAnswer: Re-rankers in search engines are models or algorithms used to re-order search results to improve their relevance to the user's query. They take into account various features and data to provide a more accurate ranking. For example, Cross Encoders use a `(query, document)` pair to output a high precision relevance score, Metadata Rankers use symbolic features like user and document attributes to predict relevance\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inspect_history(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Test question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted answer: Context:\n",
      "[1] «These rewritten queries and prompts ensure that the search process better understands and retrieves relevant documents and the language model is prompted optimally. Query rewriting can be achieved as demonstrated below. ![rewrite](./img/image9.png)\n",
      "[Query Rewriting - Ma et al. 2023](https://arxiv.org/abs/2305.142\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 0.8 / 1  (80.0):  10%|█         | 1/10 [00:00<00:05,  1.79it/s]Test question: How can I retrieve the total object count in a class?\n",
      "Predicted answer: You can retrieve the total object count in a class by using the Weaviate API to perform an object count query for the specific class. This ensures that the count is accurate and specific to the class in question.\n",
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Average Metric: 3.2 / 2  (160.0):  20%|██        | 2/10 [00:01<00:04,  1.62it/s]Test question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted answer: The cosine similarity from Weaviate's certainty can be directly obtained as they are equivalent. Certainty is a number between 0 and 1, which corresponds to the cosine similarity.\n",
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Average Metric: 5.6 / 3  (186.7):  30%|███       | 3/10 [00:01<00:04,  1.55it/s]Test question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted answer: Context:\n",
      "[1] «|\n",
      "\n",
      "By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et al. in the self-ask paper! This result was originally placed at #6 with Hybrid Search only. This is a great opportunity to preview the discussion of how LLMs use search versus humans. When humans\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 6.3999999999999995 / 4  (160.0):  40%|████      | 4/10 [00:02<00:03,  1.53it/s]Test question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted answer: Context:\n",
      "[1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data, but the main interaction for data consumption goes via GraphQL.)\n",
      "\n",
      "GraphQL still follows the same constraints as REST APIs, but data is organized into a graph using only one interface.\n",
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 2\n",
      "Average Metric: 8.6 / 5  (172.0):  50%|█████     | 5/10 [00:03<00:03,  1.60it/s]               Test question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted answer: The best way to iterate through objects is to use the collections-first approach by creating an object for each collection and then using that object\n",
      "Faithful: 3\n",
      "Detail: 2\n",
      "Overall: 3\n",
      "Average Metric: 10.8 / 6  (180.0):  60%|██████    | 6/10 [00:03<00:02,  1.67it/s]Test question: What is best practice for updating data?\n",
      "Predicted answer: The best practice for updating data depends on the system and use case. For DSPy programs, updating can\n",
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Average Metric: 12.4 / 7  (177.1):  70%|███████   | 7/10 [00:04<00:02,  1.36it/s]Test question: Can I connect my own module?\n",
      "Predicted answer: Yes, you can connect your own module by uploading it to Hugging Face as a private module and then using it in Weaviate.\n",
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Average Metric: 15.8 / 8  (197.5):  80%|████████  | 8/10 [00:05<00:01,  1.35it/s]Test question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted answer: Context: May contain relevant facts\n",
      "\n",
      "Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to determine if you can train your own `text2vec-contextionary` vectorizer module. We need to look for any mention of training custom vectorizer modules, specifically the `text2vec-context\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 3\n",
      "Average Metric: 17.0 / 9  (188.9):  90%|█████████ | 9/10 [00:06<00:00,  1.39it/s]Test question: Does Weaviate use Hnswlib?\n",
      "Predicted answer: No\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 4\n",
      "Average Metric: 18.4 / 10  (184.0): 100%|██████████| 10/10 [00:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d3cf9 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d3cf9 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d3cf9_row0_col0, #T_d3cf9_row0_col1, #T_d3cf9_row0_col2, #T_d3cf9_row1_col0, #T_d3cf9_row1_col1, #T_d3cf9_row1_col2, #T_d3cf9_row2_col0, #T_d3cf9_row2_col1, #T_d3cf9_row2_col2, #T_d3cf9_row3_col0, #T_d3cf9_row3_col1, #T_d3cf9_row3_col2, #T_d3cf9_row4_col0, #T_d3cf9_row4_col1, #T_d3cf9_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d3cf9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d3cf9_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_d3cf9_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_d3cf9_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d3cf9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d3cf9_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_d3cf9_row0_col1\" class=\"data row0 col1\" >Context: [1] «These rewritten queries and prompts ensure that the search process better understands and retrieves relevant documents and the language model is prompted optimally....</td>\n",
       "      <td id=\"T_d3cf9_row0_col2\" class=\"data row0 col2\" >✔️ [0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3cf9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d3cf9_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_d3cf9_row1_col1\" class=\"data row1 col1\" >You can retrieve the total object count in a class by using the Weaviate API to perform an object count query for the specific class....</td>\n",
       "      <td id=\"T_d3cf9_row1_col2\" class=\"data row1 col2\" >✔️ [2.4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3cf9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d3cf9_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_d3cf9_row2_col1\" class=\"data row2 col1\" >The cosine similarity from Weaviate's certainty can be directly obtained as they are equivalent. Certainty is a number between 0 and 1, which corresponds to...</td>\n",
       "      <td id=\"T_d3cf9_row2_col2\" class=\"data row2 col2\" >✔️ [2.4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3cf9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d3cf9_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_d3cf9_row3_col1\" class=\"data row3 col1\" >Context: [1] «| By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et...</td>\n",
       "      <td id=\"T_d3cf9_row3_col2\" class=\"data row3 col2\" >✔️ [0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3cf9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d3cf9_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_d3cf9_row4_col1\" class=\"data row4 col1\" >Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data,...</td>\n",
       "      <td id=\"T_d3cf9_row4_col2\" class=\"data row4 col2\" >✔️ [2.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x129911280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "184.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "evaluate(RAG(), metric = llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\n",
      "[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\n",
      "[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\n",
      "\n",
      "### Weaviate\n",
      "[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\n",
      "\n",
      "The HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\n",
      "\n",
      "Question: Does Weaviate use Hnswlib?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Reasoning: Let's think step by step in order to determine if Weaviate uses Hnswlib. We need to look for any direct mentions of Hnswlib being used by Weaviate or any custom implementations based on it.\n",
      "\n",
      "1. The context mentions that \"the most popular library hnswlib only supports snapshotting, but not individual writes to disk.\"\n",
      "2. It then states, \"To get to where Weaviate is today, a custom HNSW implementation was needed.\"\n",
      "3. This implies that while Weaviate is based on the principles of HNSW (as outlined in a specific paper), it does not directly use Hnswlib but rather a custom implementation of HNSW.\n",
      "\n",
      "Answer: No\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nContext:\\n[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\\n[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\\n[3] «Note your queries won\\'t return any objects that are not imported yet, as you can\\'t query what you don\\'t have. 🤔\\n\\n### Weaviate\\n[Weaviate](/developers/weaviate/) is an open-source vector database. If you\\'re new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\\n\\nThe HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate\\'s ANN performance for different use-cases.»\\n\\nQuestion: Does Weaviate use Hnswlib?\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m Reasoning: Let\\'s think step by step in order to determine if Weaviate uses Hnswlib. We need to look for any direct mentions of Hnswlib being used by Weaviate or any custom implementations based on it.\\n\\n1. The context mentions that \"the most popular library hnswlib only supports snapshotting, but not individual writes to disk.\"\\n2. It then states, \"To get to where Weaviate is today, a custom HNSW implementation was needed.\"\\n3. This implies that while Weaviate is based on the principles of HNSW (as outlined in a specific paper), it does not directly use Hnswlib but rather a custom implementation of HNSW.\\n\\nAnswer: No\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\n",
      "[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\n",
      "[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\n",
      "\n",
      "### Weaviate\n",
      "[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\n",
      "\n",
      "The HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\n",
      "[4] «Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.»\n",
      "[5] «![embeddings](./img/embeddings.png)\n",
      "\n",
      "At Weaviate, we’ve implemented a custom [HNSW](https://weaviate.io/developers/weaviate/concepts/vector-index#hierarchical-navigable-small-world-hnsw-index) [indexing](https://weaviate.io/developers/weaviate/concepts/vector-index) algorithm to allow for highly performant querying over your dataset which enables all sorts of business use cases at scale. ## Features for Enterprise\n",
      "\n",
      "A good vector database is one that is highly flexible and fits your needs. One that allows you to remain in control of your data at all times and gives you the control to do what you want with that data, such as store it in your own virtual private cloud when you have stricter compliance concerns. Weaviate has way too many features and capabilities to describe in a single blog post, but here are some of the most relevant ones for enterprise use cases. ### Search\n",
      "\n",
      "Weaviate’s core competency can be boiled down to search.»\n",
      "\n",
      "Assessment Criterion: Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\n",
      "\n",
      "Assessed Answer: No\n",
      "\n",
      "Reasoning: Let's think step by step in order to 1\n",
      "\n",
      "Assessment Answer:\u001b[32m 1\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\n",
      "[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\n",
      "[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\n",
      "\n",
      "### Weaviate\n",
      "[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\n",
      "\n",
      "The HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\n",
      "[4] «Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.»\n",
      "[5] «![embeddings](./img/embeddings.png)\n",
      "\n",
      "At Weaviate, we’ve implemented a custom [HNSW](https://weaviate.io/developers/weaviate/concepts/vector-index#hierarchical-navigable-small-world-hnsw-index) [indexing](https://weaviate.io/developers/weaviate/concepts/vector-index) algorithm to allow for highly performant querying over your dataset which enables all sorts of business use cases at scale. ## Features for Enterprise\n",
      "\n",
      "A good vector database is one that is highly flexible and fits your needs. One that allows you to remain in control of your data at all times and gives you the control to do what you want with that data, such as store it in your own virtual private cloud when you have stricter compliance concerns. Weaviate has way too many features and capabilities to describe in a single blog post, but here are some of the most relevant ones for enterprise use cases. ### Search\n",
      "\n",
      "Weaviate’s core competency can be boiled down to search.»\n",
      "\n",
      "Assessment Criterion: Please rate how well this answer addresses the question, `Does Weaviate use Hnswlib?` based on the context. `No`\n",
      "\n",
      "Assessed Answer: No\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m 5\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Assess the quality of an answer to a question.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: The context for answering the question\n",
      "\n",
      "Assessment Criterion: The evaluation criterion\n",
      "\n",
      "Assessed Answer: The answer to the question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\n",
      "\n",
      "Assessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\n",
      "[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\n",
      "[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\n",
      "\n",
      "### Weaviate\n",
      "[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\n",
      "\n",
      "The HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\n",
      "[4] «Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.»\n",
      "[5] «![embeddings](./img/embeddings.png)\n",
      "\n",
      "At Weaviate, we’ve implemented a custom [HNSW](https://weaviate.io/developers/weaviate/concepts/vector-index#hierarchical-navigable-small-world-hnsw-index) [indexing](https://weaviate.io/developers/weaviate/concepts/vector-index) algorithm to allow for highly performant querying over your dataset which enables all sorts of business use cases at scale. ## Features for Enterprise\n",
      "\n",
      "A good vector database is one that is highly flexible and fits your needs. One that allows you to remain in control of your data at all times and gives you the control to do what you want with that data, such as store it in your own virtual private cloud when you have stricter compliance concerns. Weaviate has way too many features and capabilities to describe in a single blog post, but here are some of the most relevant ones for enterprise use cases. ### Search\n",
      "\n",
      "Weaviate’s core competency can be boiled down to search.»\n",
      "\n",
      "Assessment Criterion: Please rate how well this answer addresses the question, `Does Weaviate use Hnswlib?` based on the context. `No`\n",
      "\n",
      "Assessed Answer: No\n",
      "\n",
      "Reasoning: Let's think step by step in order to 5\n",
      "\n",
      "Assessment Answer:\u001b[32m 4\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\\n[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\\n[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\\n\\n### Weaviate\\n[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\\n\\nThe HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\\n[4] «Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.»\\n[5] «![embeddings](./img/embeddings.png)\\n\\nAt Weaviate, we’ve implemented a custom [HNSW](https://weaviate.io/developers/weaviate/concepts/vector-index#hierarchical-navigable-small-world-hnsw-index) [indexing](https://weaviate.io/developers/weaviate/concepts/vector-index) algorithm to allow for highly performant querying over your dataset which enables all sorts of business use cases at scale. ## Features for Enterprise\\n\\nA good vector database is one that is highly flexible and fits your needs. One that allows you to remain in control of your data at all times and gives you the control to do what you want with that data, such as store it in your own virtual private cloud when you have stricter compliance concerns. Weaviate has way too many features and capabilities to describe in a single blog post, but here are some of the most relevant ones for enterprise use cases. ### Search\\n\\nWeaviate’s core competency can be boiled down to search.»\\n\\nAssessment Criterion: Is the assessed text grounded in the context? Say no if it includes significant information not in the context.\\n\\nAssessed Answer: No\\n\\nReasoning: Let's think step by step in order to 1\\n\\nAssessment Answer:\\x1b[32m 1\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\\n[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\\n[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\\n\\n### Weaviate\\n[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\\n\\nThe HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\\n[4] «Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.»\\n[5] «![embeddings](./img/embeddings.png)\\n\\nAt Weaviate, we’ve implemented a custom [HNSW](https://weaviate.io/developers/weaviate/concepts/vector-index#hierarchical-navigable-small-world-hnsw-index) [indexing](https://weaviate.io/developers/weaviate/concepts/vector-index) algorithm to allow for highly performant querying over your dataset which enables all sorts of business use cases at scale. ## Features for Enterprise\\n\\nA good vector database is one that is highly flexible and fits your needs. One that allows you to remain in control of your data at all times and gives you the control to do what you want with that data, such as store it in your own virtual private cloud when you have stricter compliance concerns. Weaviate has way too many features and capabilities to describe in a single blog post, but here are some of the most relevant ones for enterprise use cases. ### Search\\n\\nWeaviate’s core competency can be boiled down to search.»\\n\\nAssessment Criterion: Please rate how well this answer addresses the question, `Does Weaviate use Hnswlib?` based on the context. `No`\\n\\nAssessed Answer: No\\n\\nReasoning: Let's think step by step in order to\\x1b[32m 5\\x1b[0m\\n\\n\\n\\n\\n\\nAssess the quality of an answer to a question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: The context for answering the question\\n\\nAssessment Criterion: The evaluation criterion\\n\\nAssessed Answer: The answer to the question\\n\\nReasoning: Let's think step by step in order to ${produce the assessment_answer}. We ...\\n\\nAssessment Answer: A rating between 1 and 5. Only output the rating and nothing else.\\n\\n---\\n\\nContext:\\n[1] «Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.»\\n[2] «Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time.»\\n[3] «Note your queries won't return any objects that are not imported yet, as you can't query what you don't have. 🤔\\n\\n### Weaviate\\n[Weaviate](/developers/weaviate/) is an open-source vector database. If you're new to Weaviate, take a look at the [Getting Started guide](/developers/weaviate/quickstart). 🙂\\n\\nThe HNSW graph is the first implementation of an ANN algorithm supported by Weaviate. [Here](/developers/weaviate/benchmarks/ann) is a benchmark that measures Weaviate's ANN performance for different use-cases.»\\n[4] «Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.»\\n[5] «![embeddings](./img/embeddings.png)\\n\\nAt Weaviate, we’ve implemented a custom [HNSW](https://weaviate.io/developers/weaviate/concepts/vector-index#hierarchical-navigable-small-world-hnsw-index) [indexing](https://weaviate.io/developers/weaviate/concepts/vector-index) algorithm to allow for highly performant querying over your dataset which enables all sorts of business use cases at scale. ## Features for Enterprise\\n\\nA good vector database is one that is highly flexible and fits your needs. One that allows you to remain in control of your data at all times and gives you the control to do what you want with that data, such as store it in your own virtual private cloud when you have stricter compliance concerns. Weaviate has way too many features and capabilities to describe in a single blog post, but here are some of the most relevant ones for enterprise use cases. ### Search\\n\\nWeaviate’s core competency can be boiled down to search.»\\n\\nAssessment Criterion: Please rate how well this answer addresses the question, `Does Weaviate use Hnswlib?` based on the context. `No`\\n\\nAssessed Answer: No\\n\\nReasoning: Let's think step by step in order to 5\\n\\nAssessment Answer:\\x1b[32m 4\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Why would I use Weaviate as my vector database?\n",
      "Predicted answer: Context:\n",
      "[1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:12,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted answer: Context:\n",
      "[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:15,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Do you offer Weaviate as a managed service?\n",
      "Predicted answer: Context:\n",
      "[1] «---\n",
      "title: Weaviate Cloud Service Public Beta - Open Now!\n",
      "slug: wcs-public-beta\n",
      "authors: [pete]\n",
      "date: 2023-05-02\n",
      "image: ./img/hero.png\n",
      "tags: ['release']\n",
      "description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:02<00:14,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: How should I configure the size of my instance?\n",
      "Predicted answer: Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:03<00:12,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 1 examples in round 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "optimizer = BootstrapFewShot(metric = llm_metric, max_labeled_demos = 8, max_rounds = 3)\n",
    "compiled_rag = optimizer.compile(uncompiled_rag, trainset = trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context: [1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well) 1. [Metadata Rankers](#metadata-rankers) 1. [Score Rankers](#score-rankers) ## Cross Encoders Cross Encoders are one of the most'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_rag(\"What do cross encoders do?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed by ANN libraries. ## Overview\n",
      "In this article we will cover:\n",
      "\n",
      "* how ANN models enable fast & large-scale vector searches\n",
      "* where popular ANN libraries fall short\n",
      "* what Weaviate is and how it can bring your vector search needs to production\n",
      "* a glimpse of how Weaviate works under the hood\n",
      "\n",
      "## What is Weaviate?»\n",
      "[2] «However, consistent with Weaviate's commitment to creating truly open-source software, customers using the free service will always be able to access all of the Weaviate's vector database capabilities. Weaviate vector database is an example of a \"third wave\" database technology. Data is processed by a machine learning model first, and AI models help in processing, storing, and searching through the data. As a result, Weaviate excels at answering questions in natural language, but it is not limited to language; it is as adaptable to searching images or even genetic information. > \"Depending on the machine-learning model used, a \"document\"—basically a data object—in a vector database typically has anywhere from 120 to 12,800 dimensions,\" van Luijt explains.»\n",
      "[3] «The company preferred an open source platform, and since they would be indexing millions of products, they needed a solution that was both high-performing and cost-efficient at scale. ## Selecting Weaviate as the vector database of choice \n",
      "After a thorough evaluation of a handful of open and closed-source vector databases, the team decided that Weaviate was the best-fit solution for their needs. They cited the following reasons for choosing Weaviate:  \n",
      "\n",
      "* Open source, with an active community and managed cloud offering. * Comprehensive documentation and strong support for popular LLMs and multi-modal models. * Direct integration of machine learning models using a module system, with the ability to easily swap out and experiment with different models.»\n",
      "\n",
      "Question: Why would I use Weaviate as my vector database?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's think step by step in order to produce the answer. We need to identify the key benefits and features of Weaviate as a vector database from the provided context. 1. Weaviate removes many limitations imposed by traditional ANN libraries, making it more flexible for vector searching. 2. It is a truly open-source software, ensuring that all vector database capabilities are accessible even in the free service. 3. Weaviate is a \"third wave\" database technology that uses machine learning models to process, store, and search data, making it highly adaptable for various types of data including natural language, images, and genetic information. 4. It supports a wide range of dimensions for data objects, from 120 to 12,\n",
      "\n",
      "Answer: Context: [1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can try it for yourself by following this [link](https://link.weaviate.io/3LiVxqp), which is already pre-populated with the above question. Press the play button, to see the magic happen. Here is the thing, finding the correct answer in a gigantic repository of unstructured data is not the most impressive part of this demonstration (I mean, it is very impressive), but it is the 🚀 speed at which it all happens. It takes a fraction of a second for the UI to show the results. We are talking about a semantic search query, which **takes milliseconds** to find an answer in a dataset containing **28 million paragraphs**.»\n",
      "[2] «Weaviate was built to combine the speed and capabilities of ANN algorithms with the features of a database such as backups, real-time queries, persistence, and replication (part of the v1.17 release). Weaviate can be accessed through GraphQL, REST, and client libraries in multiple programming languages. ### Example Use Cases\n",
      "\n",
      "Vector databases are great to use for your application if your data is constantly changing. You can use vector search engines for e-commerce recommendations, image search, semantic similarity, and the list goes on. Weaviate just released a new module that introduces a way to represent a user's interests by drawing a graph of cross-references.»\n",
      "[3] «Check out one of our free weekly workshops to help you understand what vector databases are and how they can help you build production-ready AI apps quickly and easily. If you’re curious, here are some of the most commonly asked questions we encountered:\n",
      "\n",
      "**What’s the difference between a vector database and a graph or relational database?**\n",
      "\n",
      "Graph databases are used to identify relationships between objects, and vector databases are used to find objects\n",
      "Relational databases store the relations between tables and build indexes for the fast lookup of joined tables. Vector databases, on the other hand, store the embeddings of structured and unstructured data for the quick retrieval of the objects. **What features does Weaviate offer to help me protect data privacy?**\n",
      "\n",
      "Weaviate is designed with robust security measures to ensure it meets the requirements of enterprise environments. Weaviate has achieved SOC 2 certification and is encrypted in transit and at rest.»\n",
      "\n",
      "Question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's think step by step in order to produce the answer. We need to identify the specific features and capabilities of Weaviate mentioned in the context and compare them to what is generally known about Elasticsearch, a popular search engine. 1. **Weaviate's Capabilities**: - Combines the speed and capabilities of ANN (Approximate Nearest Neighbor) algorithms with database features like backups, real-time queries, persistence, and replication. - Can be accessed through GraphQL, REST, and client libraries in multiple programming languages. - Designed for semantic search queries, which are executed in milliseconds even in large datasets. - Suitable for applications where data is constantly changing, such as e-commerce recommendations, image search, and\n",
      "\n",
      "Answer: Context: [1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/> You can\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «---\n",
      "title: Weaviate Cloud Service Public Beta - Open Now!\n",
      "slug: wcs-public-beta\n",
      "authors: [pete]\n",
      "date: 2023-05-02\n",
      "image: ./img/hero.png\n",
      "tags: ['release']\n",
      "description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate cluster up and running within minutes. Learn more here.\"\n",
      "\n",
      "---\n",
      "![Weaviate Cloud Service Public Beta - Open Now!](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "import ReactPlayer from 'react-player/lazy'\n",
      "\n",
      "Last November, we quietly introduced the private beta of the [Weaviate Cloud Service](https://weaviate.io/pricing), a fully managed [vector database](https://weaviate.io/blog/what-is-a-vector-database) as a service. Today we are announcing a big upgrade to the Weaviate Cloud and availability of the public beta release of the service. <ReactPlayer className=\"react-player\" url='https://youtu.be/WZMFFabyC_4' controls='true' />\n",
      "\n",
      "The Weaviate Cloud enables AI application developers to use the [open source Weaviate vector database](https://weaviate.io/developers/weaviate) as a fully managed cloud service. It’s the easiest way to get a vector database cluster up and running within minutes, so you can get right to loading, vectorizing, and searching your data.»\n",
      "[2] «Immediately after signing up for a Weaviate Cloud account, you can begin creating, querying, securing, and controlling one or more databases via the Weaviate Cloud console. As a result of the automation improvements, newly created Weaviate database clusters will be ready for use within a minute or two. ![Weaviate Cloud Service Create Cluster Page ](./img/wcs-options.png)\n",
      "\n",
      "### Weaviate Cloud Service runs on Google Cloud (other clouds & on-premises are options too)\n",
      "The Weaviate Cloud Service runs on Google Cloud, and by default, new Weaviate database clusters are provisioned to run on Google Cloud. When you create a new cluster, you can choose a specific Google Cloud location (region) on which to run your database cluster. We do plan to make Weaviate Cloud available on AWS and Azure; stay tuned for news on that!\n",
      "\n",
      "### Hybrid SaaS Option - Fully Managed, on Your Cloud\n",
      "If you’d like your database hosted on a dedicated private cloud, you can contact us and [request a Hybrid-SaaS cluster](https://weaviate.io/pricing).»\n",
      "[3] «For this project, we used a cluster created through Weaviate Cloud. Weaviate Cloud is a fully managed service that allows you to deploy Weaviate in a few clicks. You can find more create a cluster on Weaviate Cloud [here](https://console.weaviate.cloud/). Once you've created a cluster on Weaviate Cloud, you'll get the Endpoint URL from your cluster as well as the API key. You'll be able to see them once you expand the details for your cluster, as shown in the following screenshot.»\n",
      "\n",
      "Question: Do you offer Weaviate as a managed service?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's think step by step in order to determine if Weaviate is offered as a managed service. We can look at the context provided to see if there are any mentions of Weaviate being available as a managed service. 1. The context mentions the \"Weaviate Cloud Service,\" which is described as a fully managed vector database as a service. 2. It states that the Weaviate Cloud enables AI application developers to use the open-source Weaviate vector database as a fully managed cloud service. 3. It also mentions that the Weaviate Cloud Service runs on Google Cloud and that there are plans to make it available on AWS and Azure. 4. Additionally, there is a mention of a Hybrid SaaS option for\n",
      "\n",
      "Answer: Context: [1] «--- title: Weaviate Cloud Service Public Beta - Open Now! slug: wcs-public-beta authors: [pete] date: 2023-05-02 image: ./img/hero.png tags: ['release'] description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should run again. That works well for applications that have tiny permanent live heaps. For example, if your constant heap is just 100MB and you have 6GB available, you can double your heap targets many times before ever being in danger.»\n",
      "[2] «Otherwise you may end up wondering why your changes are not carrying over. :::\n",
      "\n",
      "```sql\n",
      "PUT file:///YOUR/PATH/specs/ollama.yaml @yaml_stage overwrite=true auto_compress=false overwrite=true;\n",
      "```\n",
      "\n",
      "### Step 3:  Start Services\n",
      "#### Create Compute Pool\n",
      "\n",
      "Once the files were uploaded I created compute pools for all the services. For example:\n",
      "\n",
      "```sql\n",
      "USE ROLE SYSADMIN;\n",
      "CREATE COMPUTE POOL OLLAMA_POOL\n",
      " MIN_NODES = 1\n",
      " MAX_NODES = 1\n",
      " INSTANCE_FAMILY = GPU_NV_M;\n",
      "```\n",
      "\n",
      "#### GPUs\n",
      "In this example, I am using a Medium NVIDIA GPU class instance for the Ollama compute pool, a small GPU instance for the vectorizer, and two small CPU instances for Weaviate and Jupyter to run on. Ollama has the flexibility to be able to handle both CPU and GPU instances, but GPUs are best for an LLM or vectorizer. For a list of available instance types you can spin up in SPCS, check out [this page](https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool).»\n",
      "[3] «If your application load increases and temporary allocations increase, your dynamic targets would be 200MB, 400MB, 800MB, 1600MB, and 3200MB. Your load must double six times to cross the 6GB mark. In other words, running out of memory is extremely unlikely. But now, let's think back to our cache application with a permanent 4GB live heap on a 6GB machine. Even the first doubling of the heap is highly problematic because the new target (8GB) would already exceed the physical memory on the machine.»\n",
      "\n",
      "Question: How should I configure the size of my instance?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Reasoning: Let's think step by step in order to determine the appropriate instance size for your application. We need to consider the type of workload and the resources required. 1. **Understand the Workload**: If your application involves heavy computations, such as running large language models (LLMs) or vectorizers, you should consider using GPU instances. For less intensive tasks, CPU instances might suffice. 2. **Permanent vs. Temporary Allocations**: If your application has a small permanent live heap, you can afford to have a smaller instance size because the heap can grow dynamically without quickly exhausting the available memory. However, if your application has a large permanent live heap, you need to ensure that the instance size can accommodate this without exceeding physical\n",
      "\n",
      "Answer: Context: [1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\n",
      "\n",
      "Question: What do cross encoders do?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We need to identify the specific function and characteristics of cross encoders from the provided context. 1. **Function**: Cross encoders are used for content-based re-ranking. They do not produce vector embeddings for data but instead use a classification mechanism for data pairs. 2. **Input and Output**: The input to a cross encoder model always consists of a data pair (e.g., two sentences). The model outputs a value between 0 and 1, indicating the similarity between these two items. 3. **Usage in Search**: In search applications, cross encoders are used to calculate the similarity between the search query and each data item. This is done by passing each data item and the query as a pair\n",
      "\n",
      "Answer:\u001b[32m Context: [1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well) 1. [Metadata Rankers](#metadata-rankers) 1. [Score Rankers](#score-rankers) ## Cross Encoders Cross Encoders are one of the most\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nContext:\\n[1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That\\'s not how it works, right? So why should it work like this for vector searching? In this article, I\\'m going to introduce you to Weaviate, a vector database that removes many of the limitations imposed by ANN libraries. ## Overview\\nIn this article we will cover:\\n\\n* how ANN models enable fast & large-scale vector searches\\n* where popular ANN libraries fall short\\n* what Weaviate is and how it can bring your vector search needs to production\\n* a glimpse of how Weaviate works under the hood\\n\\n## What is Weaviate?»\\n[2] «However, consistent with Weaviate\\'s commitment to creating truly open-source software, customers using the free service will always be able to access all of the Weaviate\\'s vector database capabilities. Weaviate vector database is an example of a \"third wave\" database technology. Data is processed by a machine learning model first, and AI models help in processing, storing, and searching through the data. As a result, Weaviate excels at answering questions in natural language, but it is not limited to language; it is as adaptable to searching images or even genetic information. > \"Depending on the machine-learning model used, a \"document\"—basically a data object—in a vector database typically has anywhere from 120 to 12,800 dimensions,\" van Luijt explains.»\\n[3] «The company preferred an open source platform, and since they would be indexing millions of products, they needed a solution that was both high-performing and cost-efficient at scale. ## Selecting Weaviate as the vector database of choice \\nAfter a thorough evaluation of a handful of open and closed-source vector databases, the team decided that Weaviate was the best-fit solution for their needs. They cited the following reasons for choosing Weaviate:  \\n\\n* Open source, with an active community and managed cloud offering. * Comprehensive documentation and strong support for popular LLMs and multi-modal models. * Direct integration of machine learning models using a module system, with the ability to easily swap out and experiment with different models.»\\n\\nQuestion: Why would I use Weaviate as my vector database?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s think step by step in order to produce the answer. We need to identify the key benefits and features of Weaviate as a vector database from the provided context. 1. Weaviate removes many limitations imposed by traditional ANN libraries, making it more flexible for vector searching. 2. It is a truly open-source software, ensuring that all vector database capabilities are accessible even in the free service. 3. Weaviate is a \"third wave\" database technology that uses machine learning models to process, store, and search data, making it highly adaptable for various types of data including natural language, images, and genetic information. 4. It supports a wide range of dimensions for data objects, from 120 to 12,\\n\\nAnswer: Context: [1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That\\'s not how it works, right? So why should it work like this for vector searching? In this article, I\\'m going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\\n\\n---\\n\\nContext:\\n[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\\nYou can try it for yourself by following this [link](https://link.weaviate.io/3LiVxqp), which is already pre-populated with the above question. Press the play button, to see the magic happen. Here is the thing, finding the correct answer in a gigantic repository of unstructured data is not the most impressive part of this demonstration (I mean, it is very impressive), but it is the 🚀 speed at which it all happens. It takes a fraction of a second for the UI to show the results. We are talking about a semantic search query, which **takes milliseconds** to find an answer in a dataset containing **28 million paragraphs**.»\\n[2] «Weaviate was built to combine the speed and capabilities of ANN algorithms with the features of a database such as backups, real-time queries, persistence, and replication (part of the v1.17 release). Weaviate can be accessed through GraphQL, REST, and client libraries in multiple programming languages. ### Example Use Cases\\n\\nVector databases are great to use for your application if your data is constantly changing. You can use vector search engines for e-commerce recommendations, image search, semantic similarity, and the list goes on. Weaviate just released a new module that introduces a way to represent a user\\'s interests by drawing a graph of cross-references.»\\n[3] «Check out one of our free weekly workshops to help you understand what vector databases are and how they can help you build production-ready AI apps quickly and easily. If you’re curious, here are some of the most commonly asked questions we encountered:\\n\\n**What’s the difference between a vector database and a graph or relational database?**\\n\\nGraph databases are used to identify relationships between objects, and vector databases are used to find objects\\nRelational databases store the relations between tables and build indexes for the fast lookup of joined tables. Vector databases, on the other hand, store the embeddings of structured and unstructured data for the quick retrieval of the objects. **What features does Weaviate offer to help me protect data privacy?**\\n\\nWeaviate is designed with robust security measures to ensure it meets the requirements of enterprise environments. Weaviate has achieved SOC 2 certification and is encrypted in transit and at rest.»\\n\\nQuestion: What is the difference between Weaviate and for example Elasticsearch?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s think step by step in order to produce the answer. We need to identify the specific features and capabilities of Weaviate mentioned in the context and compare them to what is generally known about Elasticsearch, a popular search engine. 1. **Weaviate\\'s Capabilities**: - Combines the speed and capabilities of ANN (Approximate Nearest Neighbor) algorithms with database features like backups, real-time queries, persistence, and replication. - Can be accessed through GraphQL, REST, and client libraries in multiple programming languages. - Designed for semantic search queries, which are executed in milliseconds even in large datasets. - Suitable for applications where data is constantly changing, such as e-commerce recommendations, image search, and\\n\\nAnswer: Context: [1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/> You can\\n\\n---\\n\\nContext:\\n[1] «---\\ntitle: Weaviate Cloud Service Public Beta - Open Now!\\nslug: wcs-public-beta\\nauthors: [pete]\\ndate: 2023-05-02\\nimage: ./img/hero.png\\ntags: [\\'release\\']\\ndescription: \"The Weaviate Cloud Service is the easiest way to get a Weaviate cluster up and running within minutes. Learn more here.\"\\n\\n---\\n![Weaviate Cloud Service Public Beta - Open Now!](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nimport ReactPlayer from \\'react-player/lazy\\'\\n\\nLast November, we quietly introduced the private beta of the [Weaviate Cloud Service](https://weaviate.io/pricing), a fully managed [vector database](https://weaviate.io/blog/what-is-a-vector-database) as a service. Today we are announcing a big upgrade to the Weaviate Cloud and availability of the public beta release of the service. <ReactPlayer className=\"react-player\" url=\\'https://youtu.be/WZMFFabyC_4\\' controls=\\'true\\' />\\n\\nThe Weaviate Cloud enables AI application developers to use the [open source Weaviate vector database](https://weaviate.io/developers/weaviate) as a fully managed cloud service. It’s the easiest way to get a vector database cluster up and running within minutes, so you can get right to loading, vectorizing, and searching your data.»\\n[2] «Immediately after signing up for a Weaviate Cloud account, you can begin creating, querying, securing, and controlling one or more databases via the Weaviate Cloud console. As a result of the automation improvements, newly created Weaviate database clusters will be ready for use within a minute or two. ![Weaviate Cloud Service Create Cluster Page ](./img/wcs-options.png)\\n\\n### Weaviate Cloud Service runs on Google Cloud (other clouds & on-premises are options too)\\nThe Weaviate Cloud Service runs on Google Cloud, and by default, new Weaviate database clusters are provisioned to run on Google Cloud. When you create a new cluster, you can choose a specific Google Cloud location (region) on which to run your database cluster. We do plan to make Weaviate Cloud available on AWS and Azure; stay tuned for news on that!\\n\\n### Hybrid SaaS Option - Fully Managed, on Your Cloud\\nIf you’d like your database hosted on a dedicated private cloud, you can contact us and [request a Hybrid-SaaS cluster](https://weaviate.io/pricing).»\\n[3] «For this project, we used a cluster created through Weaviate Cloud. Weaviate Cloud is a fully managed service that allows you to deploy Weaviate in a few clicks. You can find more create a cluster on Weaviate Cloud [here](https://console.weaviate.cloud/). Once you\\'ve created a cluster on Weaviate Cloud, you\\'ll get the Endpoint URL from your cluster as well as the API key. You\\'ll be able to see them once you expand the details for your cluster, as shown in the following screenshot.»\\n\\nQuestion: Do you offer Weaviate as a managed service?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s think step by step in order to determine if Weaviate is offered as a managed service. We can look at the context provided to see if there are any mentions of Weaviate being available as a managed service. 1. The context mentions the \"Weaviate Cloud Service,\" which is described as a fully managed vector database as a service. 2. It states that the Weaviate Cloud enables AI application developers to use the open-source Weaviate vector database as a fully managed cloud service. 3. It also mentions that the Weaviate Cloud Service runs on Google Cloud and that there are plans to make it available on AWS and Azure. 4. Additionally, there is a mention of a Hybrid SaaS option for\\n\\nAnswer: Context: [1] «--- title: Weaviate Cloud Service Public Beta - Open Now! slug: wcs-public-beta authors: [pete] date: 2023-05-02 image: ./img/hero.png tags: [\\'release\\'] description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate\\n\\n---\\n\\nContext:\\n[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should run again. That works well for applications that have tiny permanent live heaps. For example, if your constant heap is just 100MB and you have 6GB available, you can double your heap targets many times before ever being in danger.»\\n[2] «Otherwise you may end up wondering why your changes are not carrying over. :::\\n\\n```sql\\nPUT file:///YOUR/PATH/specs/ollama.yaml @yaml_stage overwrite=true auto_compress=false overwrite=true;\\n```\\n\\n### Step 3:  Start Services\\n#### Create Compute Pool\\n\\nOnce the files were uploaded I created compute pools for all the services. For example:\\n\\n```sql\\nUSE ROLE SYSADMIN;\\nCREATE COMPUTE POOL OLLAMA_POOL\\n MIN_NODES = 1\\n MAX_NODES = 1\\n INSTANCE_FAMILY = GPU_NV_M;\\n```\\n\\n#### GPUs\\nIn this example, I am using a Medium NVIDIA GPU class instance for the Ollama compute pool, a small GPU instance for the vectorizer, and two small CPU instances for Weaviate and Jupyter to run on. Ollama has the flexibility to be able to handle both CPU and GPU instances, but GPUs are best for an LLM or vectorizer. For a list of available instance types you can spin up in SPCS, check out [this page](https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool).»\\n[3] «If your application load increases and temporary allocations increase, your dynamic targets would be 200MB, 400MB, 800MB, 1600MB, and 3200MB. Your load must double six times to cross the 6GB mark. In other words, running out of memory is extremely unlikely. But now, let\\'s think back to our cache application with a permanent 4GB live heap on a 6GB machine. Even the first doubling of the heap is highly problematic because the new target (8GB) would already exceed the physical memory on the machine.»\\n\\nQuestion: How should I configure the size of my instance?\\n\\nReasoning: Let\\'s think step by step in order to Reasoning: Let\\'s think step by step in order to determine the appropriate instance size for your application. We need to consider the type of workload and the resources required. 1. **Understand the Workload**: If your application involves heavy computations, such as running large language models (LLMs) or vectorizers, you should consider using GPU instances. For less intensive tasks, CPU instances might suffice. 2. **Permanent vs. Temporary Allocations**: If your application has a small permanent live heap, you can afford to have a smaller instance size because the heap can grow dynamically without quickly exhausting the available memory. However, if your application has a large permanent live heap, you need to ensure that the instance size can accommodate this without exceeding physical\\n\\nAnswer: Context: [1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should\\n\\n---\\n\\nContext:\\n[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\\n\\nQuestion: What do cross encoders do?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We need to identify the specific function and characteristics of cross encoders from the provided context. 1. **Function**: Cross encoders are used for content-based re-ranking. They do not produce vector embeddings for data but instead use a classification mechanism for data pairs. 2. **Input and Output**: The input to a cross encoder model always consists of a data pair (e.g., two sentences). The model outputs a value between 0 and 1, indicating the similarity between these two items. 3. **Usage in Search**: In search applications, cross encoders are used to calculate the similarity between the search query and each data item. This is done by passing each data item and the query as a pair\\n\\nAnswer:\\x1b[32m Context: [1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well) 1. [Metadata Rankers](#metadata-rankers) 1. [Score Rankers](#score-rankers) ## Cross Encoders Cross Encoders are one of the most\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inspect_history(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Test question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted answer: Context: [1] «These rewritten queries and prompts ensure that the search process better understands and retrieves relevant documents and the language model is prompted optimally. Query rewriting can be achieved as demonstrated below. ![rewrite](./img/image9.png) [Query Rewriting - Ma et al. 2023](https://arxiv.org/abs/2305.142\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 0.8 / 1  (80.0):  10%|█         | 1/10 [00:00<00:06,  1.48it/s]Test question: How can I retrieve the total object count in a class?\n",
      "Predicted answer: To retrieve the total object count in a class, you can perform an object count operation after importing the objects. This can be done using the appropriate API or client library methods provided by Weaviate. The exact method may vary depending on the programming language or client\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Average Metric: 2.4000000000000004 / 2  (120.0):  20%|██        | 2/10 [00:01<00:07,  1.08it/s]Test question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted answer: To get the cosine similarity from Weaviate's certainty, you can use the certainty value directly as it is designed to work with cosine\n",
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 4\n",
      "Average Metric: 4.4 / 3  (146.7):  30%|███       | 3/10 [00:02<00:04,  1.40it/s]               Test question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted answer: Context: [1] «|\n",
      "\n",
      "By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et al. in the self-ask paper! This result was originally placed at #6 with Hybrid Search only. This is a great opportunity to preview the discussion of how LLMs use search versus humans. When humans\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 5.2 / 4  (130.0):  40%|████      | 4/10 [00:02<00:03,  1.55it/s]Test question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted answer: Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data, but the main interaction for data consumption goes via GraphQL.)\n",
      "\n",
      "GraphQL still follows the same constraints as REST APIs, but data is organized into a graph using only one interface.\n",
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 2\n",
      "Average Metric: 7.4 / 5  (148.0):  50%|█████     | 5/10 [00:03<00:03,  1.38it/s]Test question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted answer: Context: [1] «:::\n",
      "\n",
      ":::note What you will be able to do using the Web bundle\n",
      "Only Read operations powered by GraphQL. :::\n",
      "\n",
      "### Collections-first approach\n",
      "\n",
      "The other big change is that the `collections` client focuses on individual collections for interaction. This means that you will no longer need to specify the collection name in every request. Instead, you\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 8.200000000000001 / 6  (136.7):  60%|██████    | 6/10 [00:04<00:03,  1.33it/s]Test question: What is best practice for updating data?\n",
      "Predicted answer: The best practice for updating data is to use a system that supports real-time updates and mutability, allowing for operations such as updating\n",
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Average Metric: 10.200000000000001 / 7  (145.7):  70%|███████   | 7/10 [00:05<00:02,  1.08it/s]Test question: Can I connect my own module?\n",
      "Predicted answer: Yes, you can connect your own module to Weaviate by uploading your custom-trained models to Hugging Face as private modules and then using them within Weaviate.\n",
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Average Metric: 13.600000000000001 / 8  (170.0):  80%|████████  | 8/10 [00:06<00:01,  1.06it/s]Test question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted answer: The context does not provide specific information about training a custom text2vec\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Average Metric: 15.200000000000001 / 9  (168.9):  90%|█████████ | 9/10 [00:07<00:00,  1.06it/s]Test question: Does Weaviate use Hnswlib?\n",
      "Predicted answer: No, Weaviate does not use hnswlib directly. Instead, it uses a custom HNSW implementation that extends the principles of hnswlib with additional features\n",
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Average Metric: 18.8 / 10  (188.0): 100%|██████████| 10/10 [00:08<00:00,  1.15it/s]            \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d472d th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d472d td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d472d_row0_col0, #T_d472d_row0_col1, #T_d472d_row0_col2, #T_d472d_row1_col0, #T_d472d_row1_col1, #T_d472d_row1_col2, #T_d472d_row2_col0, #T_d472d_row2_col1, #T_d472d_row2_col2, #T_d472d_row3_col0, #T_d472d_row3_col1, #T_d472d_row3_col2, #T_d472d_row4_col0, #T_d472d_row4_col1, #T_d472d_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d472d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d472d_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_d472d_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_d472d_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d472d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d472d_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_d472d_row0_col1\" class=\"data row0 col1\" >Context: [1] «These rewritten queries and prompts ensure that the search process better understands and retrieves relevant documents and the language model is prompted optimally....</td>\n",
       "      <td id=\"T_d472d_row0_col2\" class=\"data row0 col2\" >✔️ [0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d472d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d472d_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_d472d_row1_col1\" class=\"data row1 col1\" >To retrieve the total object count in a class, you can perform an object count operation after importing the objects. This can be done using...</td>\n",
       "      <td id=\"T_d472d_row1_col2\" class=\"data row1 col2\" >✔️ [1.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d472d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d472d_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_d472d_row2_col1\" class=\"data row2 col1\" >To get the cosine similarity from Weaviate's certainty, you can use the certainty value directly as it is designed to work with cosine</td>\n",
       "      <td id=\"T_d472d_row2_col2\" class=\"data row2 col2\" >✔️ [2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d472d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d472d_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_d472d_row3_col1\" class=\"data row3 col1\" >Context: [1] «| By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et...</td>\n",
       "      <td id=\"T_d472d_row3_col2\" class=\"data row3 col2\" >✔️ [0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d472d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d472d_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_d472d_row4_col1\" class=\"data row4 col1\" >Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data,...</td>\n",
       "      <td id=\"T_d472d_row4_col2\" class=\"data row4 col2\" >✔️ [2.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x129b53950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "188.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to bootstrap 2 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Why would I use Weaviate as my vector database?\n",
      "Predicted answer: Context:\n",
      "[1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\n",
      "Test question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted answer: Context:\n",
      "[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8 / 1  (80.0):   5%|▌         | 1/20 [00:00<00:10,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.6 / 2  (80.0):  10%|█         | 2/20 [00:00<00:06,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Do you offer Weaviate as a managed service?\n",
      "Predicted answer: Context:\n",
      "[1] «---\n",
      "title: Weaviate Cloud Service Public Beta - Open Now!\n",
      "slug: wcs-public-beta\n",
      "authors: [pete]\n",
      "date: 2023-05-02\n",
      "image: ./img/hero.png\n",
      "tags: ['release']\n",
      "description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 3  (133.3):  15%|█▌        | 3/20 [00:01<00:06,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How should I configure the size of my instance?\n",
      "Predicted answer: Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should\n",
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.6 / 4  (140.0):  20%|██        | 4/20 [00:01<00:04,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted answer: While it is not strictly necessary to have prior knowledge of Docker (Compose) to use Weaviate, understanding these technologies can be very helpful. The provided resources and guides aim to make it easier for users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.0 / 5  (160.0):  25%|██▌       | 5/20 [00:05<00:23,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted answer: Context:\n",
      "[1] «:::\n",
      "\n",
      "## Implications for database maintenance\n",
      "\n",
      "In production, this can dramatically reduce the critical downtime. Let’s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions. Each Weaviate pod will restart one by one, as demonstrated in the example\n",
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted answer: Yes, there are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.0 / 6  (166.7):  30%|███       | 6/20 [00:08<00:29,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 1\n",
      "Test question: Should I use references in my schema?\n",
      "Predicted answer: Yes, you should consider using references in your schema if you need to link related data across different classes, as it can enhance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.8 / 7  (154.3):  35%|███▌      | 7/20 [00:10<00:29,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted answer: Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data, but the main interaction for data consumption goes via GraphQL.) GraphQL still follows the same constraints as REST APIs, but data is organized into a graph using only one interface.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.200000000000001 / 8  (152.5):  40%|████      | 8/20 [00:16<00:38,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 3\n",
      "Test question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted answer: The context does not explicitly define `valueText` and `valueString`, but it does mention `string` and `text` data types. In Weaviate, `string` and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.600000000000001 / 9  (151.1):  45%|████▌     | 9/20 [00:22<00:45,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 2\n",
      "Test question: Do Weaviate classes have namespaces?\n",
      "Predicted answer: Yes, Weaviate classes have namespaces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.200000000000001 / 10  (152.0):  50%|█████     | 10/20 [00:45<01:40, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted answer: The context does not provide explicit information about restrictions on UUID formatting or adherence to any specific standards.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.6 / 11  (160.0):  55%|█████▌    | 11/20 [00:53<01:24,  9.39s/it]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted answer: The context does not explicitly state whether Weaviate will create a UUID automatically if one is not specified during the addition of data objects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.8 / 12  (173.3):  60%|██████    | 12/20 [00:59<01:06,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.0 / 13  (169.2):  65%|██████▌   | 13/20 [01:01<00:45,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 1\n",
      "Test question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted answer: Context:\n",
      "[1] «The very first iteration of Weaviate focused on exactly this: \"Could Weave be used to define other things than IoT devices like transactions, or cars, or any other things?\". In 2017 Google deprecated Weave and renamed Brillo to Android Things but the concept for Weaviate stayed. From the get-go, I knew\n",
      "Test question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted answer: Context:\n",
      "[1] «One of the most important upsides of this approach was that we could use GraphQL (the graph query language which was entering the software stage through Facebook open-sourcing it) to represent the data inside Weaviate. With the concept of realtime vectorization of data objects and RDF-like representation of Weaviate objects in GraphQL, all the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.0 / 14  (164.3):  70%|███████   | 14/20 [01:09<00:41,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.8 / 15  (158.7):  75%|███████▌  | 15/20 [01:09<00:24,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: How to deal with custom terminology?\n",
      "Predicted answer: To deal with custom terminology, it is important to have control over the context and sources of information. Using an application like Verba can help, as it allows you to manage what the system knows and see the sources of its\n",
      "Test question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted answer: Reasoning: Let's think step by step in order to produce the answer. We need to understand the context provided about Weaviate's data schema and compare it with general concepts of ontologies and taxonomies.\n",
      "\n",
      "1. **Weaviate Data Schema**: According to the context, the Weaviate data schema is used to define data types, vectorizers, and cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.8 / 16  (161.2):  80%|████████  | 16/20 [01:15<00:20,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.0 / 17  (158.8):  85%|████████▌ | 17/20 [01:17<00:12,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 3\n",
      "Test question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted answer: You can index data near-realtime without losing semantic meaning by using a vector database that indexes data based on data vectors or vector embeddings. This method allows for near real-time\n",
      "Test question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted answer: The reason there isn't a text2vec-contextionary in your language could be because the specific embedding model or vectorizer module you are using does not support your language. It is important to check the documentation of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.6 / 18  (164.4):  90%|█████████ | 18/20 [01:32<00:14,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test question: How do you deal with words that have multiple meanings?\n",
      "Predicted answer: To deal with words that have multiple meanings, transformer models such as BERT create contextual embeddings by considering the entire input text. Each occurrence of a word is given its own embedding that is modified by the surrounding text, which helps in disambiguating the word's meaning based on its context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28668, Requested 2403. Please try again in 2.142s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 33.4 / 19  (175.8):  95%|█████████▌| 19/20 [01:42<00:08,  8.26s/it]INFO:backoff:Backing off request(...) for 1.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28977, Requested 2403. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 35.4 / 20  (177.0): 100%|██████████| 20/20 [01:47<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Score: 177.0 for set: [0]\n",
      "New best sscore: 177.0 for seed -3\n",
      "Scores so far: [177.0]\n",
      "Best score: 177.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What is the difference between Weaviate and for example Elasticsearch?Test question: Why would I use Weaviate as my vector database?\n",
      "Predicted answer: Context:\n",
      "[1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\n",
      "\n",
      "Predicted answer: Context:\n",
      "[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.6 / 2  (80.0):   5%|▌         | 1/20 [00:00<00:13,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: How should I configure the size of my instance?Test question: Do you offer Weaviate as a managed service?\n",
      "Predicted answer: Context:\n",
      "[1] «---\n",
      "title: Weaviate Cloud Service Public Beta - Open Now!\n",
      "slug: wcs-public-beta\n",
      "authors: [pete]\n",
      "date: 2023-05-02\n",
      "image: ./img/hero.png\n",
      "tags: ['release']\n",
      "description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate\n",
      "\n",
      "Predicted answer: Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.6 / 4  (140.0):  15%|█▌        | 3/20 [00:01<00:07,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted answer: While it is not strictly necessary to have prior knowledge of Docker (Compose) to use Weaviate, understanding these technologies can be very helpful. The provided resources and guides aim to make it easier for users\n",
      "Test question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted answer: Context:\n",
      "[1] «:::\n",
      "\n",
      "## Implications for database maintenance\n",
      "\n",
      "In production, this can dramatically reduce the critical downtime. Let’s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions. Each Weaviate pod will restart one by one, as demonstrated in the example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.0 / 5  (160.0):  25%|██▌       | 5/20 [00:02<00:05,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.0 / 6  (166.7):  30%|███       | 6/20 [00:02<00:05,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted answer: Yes, there are\n",
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.8 / 7  (154.3):  35%|███▌      | 7/20 [00:02<00:04,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Should I use references in my schema?\n",
      "Predicted answer: Yes, you should consider using references in your schema if you need to link related data across different classes, as it can enhance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.200000000000001 / 8  (152.5):  40%|████      | 8/20 [00:02<00:04,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted answer: Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data, but the main interaction for data consumption goes via GraphQL.) GraphQL still follows the same constraints as REST APIs, but data is organized into a graph using only one interface.\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.4 / 9  (148.9):  45%|████▌     | 9/20 [00:03<00:03,  3.27it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1Test question: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
      "Predicted answer: The context does not explicitly define `valueText` and `valueString`, but it does mention `string` and `text` data types. In Weaviate, `string` and\n",
      "\n",
      "Detail: 3\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.8 / 10  (148.0):  50%|█████     | 10/20 [00:03<00:03,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Do Weaviate classes have namespaces?\n",
      "Predicted answer: Yes, Weaviate classes have namespaces.\n",
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.400000000000002 / 11  (149.1):  55%|█████▌    | 11/20 [00:03<00:02,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted answer: The context does not provide explicit information about restrictions on UUID formatting or adherence to any specific standards.\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.8 / 12  (156.7):  60%|██████    | 12/20 [00:04<00:02,  3.23it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted answer: The context does not explicitly state whether Weaviate will create a UUID automatically if one is not specified during the addition of data objects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.0 / 13  (169.2):  65%|██████▌   | 13/20 [00:04<00:02,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted answer: Context:\n",
      "[1] «One of the most important upsides of this approach was that we could use GraphQL (the graph query language which was entering the software stage through Facebook open-sourcing it) to represent the data inside Weaviate. With the concept of realtime vectorization of data objects and RDF-like representation of Weaviate objects in GraphQL, all the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.8 / 14  (162.9):  70%|███████   | 14/20 [00:04<00:01,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted answer: Context:\n",
      "[1] «The very first iteration of Weaviate focused on exactly this: \"Could Weave be used to define other things than IoT devices like transactions, or cars, or any other things?\". In 2017 Google deprecated Weave and renamed Brillo to Android Things but the concept for Weaviate stayed. From the get-go, I knew\n",
      "Test question: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
      "Predicted answer: Reasoning: Let's think step by step in order to produce the answer. We need to understand the context provided about Weaviate's data schema and compare it with general concepts of ontologies and taxonomies.\n",
      "\n",
      "1. **Weaviate Data Schema**: According to the context, the Weaviate data schema is used to define data types, vectorizers, and cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.0 / 16  (156.2):  80%|████████  | 16/20 [00:05<00:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 3\n",
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n",
      "Test question: How to deal with custom terminology?\n",
      "Predicted answer: To deal with custom terminology, it is important to have control over the context and sources of information. Using an application like Verba can help, as it allows you to manage what the system knows and see the sources of its\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.0 / 17  (158.8):  85%|████████▌ | 17/20 [00:06<00:01,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted answer: You can index data near-realtime without losing semantic meaning by using a vector database that indexes data based on data vectors or vector embeddings. This method allows for near real-time\n",
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.6 / 18  (164.4):  90%|█████████ | 18/20 [00:06<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Test question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted answer: The reason there isn't a text2vec-contextionary in your language could be because the specific embedding model or vectorizer module you are using does not support your language. It is important to check the documentation of\n",
      "Test question: How do you deal with words that have multiple meanings?\n",
      "Predicted answer: To deal with words that have multiple meanings, transformer models such as BERT create contextual embeddings by considering the entire input text. Each occurrence of a word is given its own embedding that is modified by the surrounding text, which helps in disambiguating the word's meaning based on its context.\n",
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 35.4 / 20  (177.0): 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 4\n",
      "Overall: 5\n",
      "Score: 177.0 for set: [4]\n",
      "Scores so far: [177.0, 177.0]\n",
      "Best score: 177.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Why would I use Weaviate as my vector database?\n",
      "Predicted answer: Context:\n",
      "[1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:19,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted answer: Context:\n",
      "[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:14,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Do you offer Weaviate as a managed service?\n",
      "Predicted answer: Context:\n",
      "[1] «---\n",
      "title: Weaviate Cloud Service Public Beta - Open Now!\n",
      "slug: wcs-public-beta\n",
      "authors: [pete]\n",
      "date: 2023-05-02\n",
      "image: ./img/hero.png\n",
      "tags: ['release']\n",
      "description: \"The Weaviate Cloud Service is the easiest way to get a Weaviate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:02<00:12,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: How should I configure the size of my instance?\n",
      "Predicted answer: Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC should\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:03<00:12,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Why would I use Weaviate as my vector database?\n",
      "Predicted answer: Context: [1] «Imagine you were using a MySQL database, but after you imported your data it would be read-only for the rest of time. That's not how it works, right? So why should it work like this for vector searching? In this article, I'm going to introduce you to Weaviate, a vector database that removes many of the limitations imposed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8 / 1  (80.0):   5%|▌         | 1/20 [00:05<01:51,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29418, Requested 4478. Please try again in 7.791s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted answer: Context: [1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.6 / 2  (80.0):  10%|█         | 2/20 [00:21<03:29, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28915, Requested 4478. Please try again in 6.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:04:15.162354Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28915, Requested 4478. Please try again in 6.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 1.6 / 3  (53.3):  15%|█▌        | 3/20 [00:36<03:42, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How should I configure the size of my instance?\n",
      "Predicted answer: ---\n",
      "\n",
      "Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.4000000000000004 / 4  (60.0):  20%|██        | 4/20 [00:43<02:51, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29310, Requested 4443. Please try again in 7.505s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted answer: When the Weaviate Docker container restarts, your data in the Weaviate database is not necessarily lost. Weaviate supports data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 5  (80.0):  25%|██▌       | 5/20 [01:02<03:25, 13.70s/it]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28151, Requested 4443. Please try again in 5.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:04:43.378309Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28151, Requested 4443. Please try again in 5.188s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 4.0 / 6  (66.7):  30%|███       | 6/20 [01:04<02:16,  9.77s/it]INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28914, Requested 4259. Please try again in 6.346s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Should I use references in my schema?\n",
      "Predicted answer: Context: [1] «The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weav\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.0 / 7  (71.4):  35%|███▌      | 7/20 [01:25<02:55, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 26635, Requested 4259. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:05:09.513812Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 26635, Requested 4259. Please try again in 1.788s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 5.0 / 8  (62.5):  40%|████      | 8/20 [01:30<02:09, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted answer: Yes, it is possible to create one-to-many relationships in the schema in Weaviate. The data objects in Weaviate are connected in a graph format, which supports such relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.6 / 9  (84.4):  45%|████▌     | 9/20 [01:41<01:57, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28911, Requested 4488. Please try again in 6.798s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Do Weaviate classes have namespaces?\n",
      "Predicted answer: Yes, Weaviate classes have namespaces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.2 / 10  (92.0):  50%|█████     | 10/20 [01:50<01:41, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29129, Requested 4488. Please try again in 7.234s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:05:35.958421Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29129, Requested 4488. Please try again in 7.234s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 9.2 / 11  (83.6):  55%|█████▌    | 11/20 [01:57<01:22,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted answer: The context does not explicitly mention any restrictions or standards for UUID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.0 / 12  (91.7):  60%|██████    | 12/20 [02:03<01:06,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 3\n",
      "Test question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted answer: The context does not explicitly state whether Weaviate will automatically create a UUID if one is not specified during the addition of data objects. However, it is likely that Weaviate follows the common practice of auto-generating unique identifiers in such cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.6 / 13  (96.9):  65%|██████▌   | 13/20 [02:18<01:11, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28911, Requested 4388. Please try again in 6.598s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29444, Requested 4388. Please try again in 7.664s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:06:12.762969Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29444, Requested 4388. Please try again in 7.664s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 12.6 / 14  (90.0):  70%|███████   | 14/20 [02:34<01:11, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted answer: Context: [1] «The very first iteration of Weaviate focused on exactly this: \"Could Weave be used to define other things than IoT devices like transactions, or cars, or any other things?\". In 2017 Google deprecated Weave and renamed Brillo to Android Things but the concept for Weaviate stayed. From the get-go, I knew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.6 / 15  (90.7):  75%|███████▌  | 15/20 [02:34<00:42,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28804, Requested 4404. Please try again in 6.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How to deal with custom terminology?\n",
      "Predicted answer: Context: [1] «- Unclarity on where the answer is coming from, possibly getting information from non-credible sources. - Misunderstanding the terminology or meaning of the users query because of the wide scope of the training data. This is where RAG can take things to the next level, especially with an application like Verba. If Verba doesn't have\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.2 / 16  (95.0):  80%|████████  | 16/20 [02:55<00:49, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29757, Requested 4404. Please try again in 8.322s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:06:41.748622Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29757, Requested 4404. Please try again in 8.322s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 15.2 / 17  (89.4):  85%|████████▌ | 17/20 [03:02<00:32, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted answer: Context: [1] «This turns out to work as well! Of course, through time, the centroid calculation algorithm in Weaviate has become way more sophisticated, but the overall concept is still the same. > By validating the above two assumptions, we knew that we could almost instantly store data objects in a semantic space rather than a more traditional row-column structure or graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28497, Requested 4538. Please try again in 6.07s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.4 / 18  (91.1):  90%|█████████ | 18/20 [03:19<00:24, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Test question: How do you deal with words that have multiple meanings?\n",
      "Predicted answer: Transformer models such as BERT and ELMo deal with words that have multiple meanings by creating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28536, Requested 4538. Please try again in 6.148s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:07:13.178554Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28536, Requested 4538. Please try again in 6.148s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 17.599999999999998 / 20  (88.0): 100%|██████████| 20/20 [03:35<00:00, 10.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Score: 88.0 for set: [4]\n",
      "Scores so far: [177.0, 177.0, 88.0]\n",
      "Best score: 177.0\n",
      "Average of max per entry across top 1 scores: 1.77\n",
      "Average of max per entry across top 2 scores: 1.77\n",
      "Average of max per entry across top 3 scores: 1.8399999999999999\n",
      "Average of max per entry across top 5 scores: 1.8399999999999999\n",
      "Average of max per entry across top 8 scores: 1.8399999999999999\n",
      "Average of max per entry across top 9999 scores: 1.8399999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Do Weaviate classes have namespaces?\n",
      "Predicted answer: Yes, Weaviate classes have namespaces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:14,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: Why isn't there a text2vec-contextionary in my language?\n",
      "Predicted answer: The reason there isn't a text2vec-contextionary in your language could be because the specific embedding model or vectorizer module you are using does not support your language. It is important to check the documentation of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:18,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Test question: How to deal with custom terminology?\n",
      "Predicted answer: To deal with custom terminology, it is important to have control over the context and sources of information. Using an application like Verba can help, as it allows you to manage what the system knows and see the sources of its\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:02<00:15,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Test question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted answer: Context:\n",
      "[1] «The very first iteration of Weaviate focused on exactly this: \"Could Weave be used to define other things than IoT devices like transactions, or cars, or any other things?\". In 2017 Google deprecated Weave and renamed Brillo to Android Things but the concept for Weaviate stayed. From the get-go, I knew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:03<00:13,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n",
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29450, Requested 3818. Please try again in 6.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Why would I use Weaviate as my vector database?\n",
      "Predicted answer: Reasoning: Let's think step by step in order to produce the answer. We need to consider the context provided and identify the relevant information that addresses the question about why one would use Weaviate as a vector database.\n",
      "\n",
      "1. **Open Source and Community Support**: Context [3] mentions that Weaviate is open source, has an active community, and offers a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.4 / 1  (240.0):   5%|▌         | 1/20 [00:18<05:51, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28110, Requested 3818. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29511, Requested 3848. Please try again in 6.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted answer: Context:\n",
      "[1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.2 / 2  (160.0):  10%|█         | 2/20 [00:32<04:45, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29015, Requested 3848. Please try again in 5.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:08:02.509655Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29015, Requested 3848. Please try again in 5.726s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 3.2 / 3  (106.7):  15%|█▌        | 3/20 [00:45<04:04, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How should I configure the size of my instance?\n",
      "Predicted answer: ---\n",
      "\n",
      "Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.0 / 4  (100.0):  20%|██        | 4/20 [00:45<02:22,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted answer: When the Weav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28483, Requested 3812. Please try again in 4.59s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.8 / 5  (96.0):  25%|██▌       | 5/20 [01:02<02:54, 11.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29718, Requested 3812. Please try again in 7.06s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:08:25.785113Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29718, Requested 3812. Please try again in 7.06s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 4.8 / 6  (80.0):  30%|███       | 6/20 [01:08<02:17,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted answer: Context: [1] «The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28534, Requested 3505. Please try again in 4.077s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.8 / 7  (82.9):  35%|███▌      | 7/20 [01:22<02:24, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n",
      "Test question: Should I use references in my schema?\n",
      "Predicted answer: Yes, you should use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28612, Requested 3616. Please try again in 4.456s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.6 / 8  (82.5):  40%|████      | 8/20 [01:35<02:21, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28830, Requested 3857. Please try again in 5.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted answer: Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data, but the main interaction for data consumption goes via GraphQL.)\n",
      "\n",
      "GraphQL still follows the same constraints as REST APIs, but data is organized into a graph using only one interface.\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.8 / 9  (86.7):  45%|████▌     | 9/20 [01:49<02:17, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 1\n",
      "Test question: Do Weaviate classes have namespaces?\n",
      "Predicted answer: Yes, Weaviate classes have namespaces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.4 / 10  (94.0):  50%|█████     | 10/20 [01:58<01:54, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted answer: The context does not provide specific information about restrictions on UUID formatting or adherence to standards. Therefore, it is unclear if there are any specific requirements for UUID formatting in Weaviate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28672, Requested 3857. Please try again in 5.058s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:09:19.320447Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28672, Requested 3857. Please try again in 5.058s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 11.4 / 12  (95.0):  60%|██████    | 12/20 [02:08<01:06,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted answer: The provided context does not explicitly state whether Weaviate will automatically create a UUID if one is not specified during the addition of data objects. Therefore, additional documentation or resources would need to be consulted to confirm this behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29782, Requested 3757. Please try again in 7.078s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 13  (115.4):  65%|██████▌   | 13/20 [02:22<01:09,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29812, Requested 3757. Please try again in 7.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:09:45.528391Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29812, Requested 3757. Please try again in 7.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 15.0 / 14  (107.1):  70%|███████   | 14/20 [02:28<00:52,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted answer: Context: [1] «The very first iteration of Weaviate focused on exactly this: \"Could Weave be used to define other things than IoT devices like transactions, or cars, or any other things?\". In 2017 Google deprecated Weave and renamed Brillo to Android Things but the concept for Weaviate stayed. From the get-go, I knew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.0 / 15  (106.7):  75%|███████▌  | 15/20 [02:36<00:42,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29158, Requested 3773. Please try again in 5.862s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: How to deal with custom terminology?\n",
      "Predicted answer: To deal with custom terminology, it is important to have control over the context and sources of information. Using an application like Verba can help, as it allows you to manage what the system knows and see the sources of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.4 / 16  (115.0):  80%|████████  | 16/20 [02:48<00:38,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29418, Requested 3773. Please try again in 6.382s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:10:12.317789Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29418, Requested 3773. Please try again in 6.382s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 18.4 / 17  (108.2):  85%|████████▌ | 17/20 [02:54<00:26,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted answer: You can index data near-realtime without losing semantic meaning by using\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.599999999999998 / 18  (108.9):  90%|█████████ | 18/20 [03:01<00:16,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29015, Requested 3907. Please try again in 5.844s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: How do you deal with words that have multiple meanings?\n",
      "Predicted answer: To deal with words that have multiple meanings, transformer models like BERT are used. These models create full contextual embeddings by taking the entire input text into account, allowing each occurrence of a word to have its own embedding that is modified by the surrounding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.799999999999997 / 19  (120.0):  95%|█████████▌| 19/20 [03:18<00:10, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 4\n",
      "Detail: 3\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 2 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 27262, Requested 3907. Please try again in 2.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:10:36.704234Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 27262, Requested 3907. Please try again in 2.338s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 22.799999999999997 / 20  (114.0): 100%|██████████| 20/20 [03:19<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 114.0 for set: [4]\n",
      "Scores so far: [177.0, 177.0, 88.0, 114.0]\n",
      "Best score: 177.0\n",
      "Average of max per entry across top 1 scores: 1.77\n",
      "Average of max per entry across top 2 scores: 1.77\n",
      "Average of max per entry across top 3 scores: 1.9\n",
      "Average of max per entry across top 5 scores: 1.97\n",
      "Average of max per entry across top 8 scores: 1.97\n",
      "Average of max per entry across top 9999 scores: 1.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted answer: The context does not provide explicit information about restrictions on UUID formatting or adherence to any specific standards.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:13,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 5\n",
      "Test question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted answer: Context:\n",
      "[1] «:::\n",
      "\n",
      "## Implications for database maintenance\n",
      "\n",
      "In production, this can dramatically reduce the critical downtime. Let’s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions. Each Weaviate pod will restart one by one, as demonstrated in the example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:14,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 1\n",
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: What is the difference between Weaviate and for example Elasticsearch?\n",
      "Predicted answer: Context: [1] «For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo – [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\n",
      "You can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8 / 1  (80.0):   5%|▌         | 1/20 [00:08<02:33,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29581, Requested 2472. Please try again in 4.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Test question: Do you offer Weaviate as a managed service?\n",
      "Predicted answer: Yes, Weaviate is offered as\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29782, Requested 2472. Please try again in 4.508s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.8 / 2  (90.0):  10%|█         | 2/20 [00:23<03:38, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 3 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29597, Requested 2472. Please try again in 4.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:11:08.194644Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29597, Requested 2472. Please try again in 4.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 1.8 / 3  (60.0):  15%|█▌        | 3/20 [00:29<02:45,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How should I configure the size of my instance?\n",
      "Predicted answer: ---\n",
      "\n",
      "Context:\n",
      "[1] «However, before Go 1.19, you only had a single knob to turn: the GOGC environment variable. This variable accepted a relative target compared to the current live heap size. The default value for GOGC is 100, meaning that the heap should double (i.e. grow by 100 percent) before GC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.6 / 4  (65.0):  20%|██        | 4/20 [00:31<01:42,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29695, Requested 2459. Please try again in 4.308s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
      "Predicted answer: ---\n",
      "\n",
      "Context:\n",
      "[1] «:::\n",
      "\n",
      "## Implications for database maintenance\n",
      "\n",
      "In production, this can dramatically reduce the critical downtime. Let’s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions. Each Weaviate pod will restart one by one, as demonstrated in the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29775, Requested 2459. Please try again in 4.468s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29667, Requested 2532. Please try again in 4.398s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Do I need to know about Docker (Compose) to use Weaviate?\n",
      "Predicted answer: Context: The context explains that Docker Compose is a tool that can run custom manifests, which is helpful for tying individual services together, especially when an application's architecture consists of individual microservices. This is relevant for Weaviate, particularly when multiple modules are involved. The context also mentions that Weaviate relies on Docker and Docker Compose to manage its services and provides guides for installing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28495, Requested 2532. Please try again in 2.054s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 3 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29794, Requested 2532. Please try again in 4.652s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:11:41.430105Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29794, Requested 2532. Please try again in 4.652s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 4.800000000000001 / 6  (80.0):  30%|███       | 6/20 [01:05<02:33, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29092, Requested 2275. Please try again in 2.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Should I use references in my schema?\n",
      "Predicted answer: Context: The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29703, Requested 2275. Please try again in 3.956s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.6000000000000005 / 7  (80.0):  35%|███▌      | 7/20 [01:20<02:41, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29792, Requested 2263. Please try again in 4.11s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
      "Predicted answer: Context:\n",
      "[1] «The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.6000000000000005 / 8  (82.5):  40%|████      | 8/20 [01:29<02:15, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29497, Requested 2504. Please try again in 4.001s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Is it possible to create one-to-many relationships in the schema?\n",
      "Predicted answer: Context: The context indicates that Weaviate uses a GraphQL schema for data interaction, and GraphQL is capable of defining relationships between data objects.\n",
      "\n",
      "Question: Is it possible to create one-to-many relationships in the schema?\n",
      "\n",
      "Reasoning: Let's think step by step in order to determine if it is possible to create one-to-many relationships in the schema. We need to examine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29892, Requested 2504. Please try again in 4.792s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.4 / 9  (93.3):  45%|████▌     | 9/20 [01:45<02:20, 12.76s/it]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 3 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29521, Requested 2504. Please try again in 4.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:12:31.545267Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29521, Requested 2504. Please try again in 4.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 8.4 / 10  (84.0):  50%|█████     | 10/20 [01:53<01:52, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: Do Weaviate classes have namespaces?\n",
      "Predicted answer: Context: [1] «There was however one point in the API where reusing IDs between classes was causing serious issues. Most noticeable this was for the [v1/objects/{id}](/developers/weaviate/api/rest#tag/objects) REST endpoints. If you wanted to retrieve, modify or delete a data object by its ID, you would just need to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28524, Requested 2268. Please try again in 1.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28621, Requested 2268. Please try again in 1.778s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.200000000000001 / 11  (83.6):  55%|█████▌    | 11/20 [02:01<01:33, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Test question: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
      "Predicted answer: The context does not provide explicit information about restrictions on UUID formatting or adherence to any specific standards.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.600000000000001 / 12  (96.7):  60%|██████    | 12/20 [02:06<01:09,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 3\n",
      "Detail: 1\n",
      "Overall: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29735, Requested 2277. Please try again in 4.024s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: Can I use Weaviate to create a traditional knowledge graph?\n",
      "Predicted answer: Context:\n",
      "[1] «One of the most important upsides of this approach was that we could use GraphQL (the graph query language which was entering the software stage through Facebook open-sourcing it) to represent the data inside Weaviate. With the concept of realtime vectorization of data objects and RDF-like representation of Weaviate objects in GraphQL, all the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.400000000000002 / 13  (95.4):  65%|██████▌   | 13/20 [02:14<00:59,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29619, Requested 2277. Please try again in 3.792s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
      "Predicted answer: The context does not provide explicit information about whether Weaviate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29092, Requested 2432. Please try again in 3.048s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.600000000000001 / 14  (97.1):  70%|███████   | 14/20 [02:29<01:02, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29276, Requested 2432. Please try again in 3.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Test question: Why does Weaviate have a schema and not an ontology?\n",
      "Predicted answer: Context: [1] «The very first iteration of Weaviate focused on exactly this: \"Could Weave be used to define other things than IoT devices like transactions, or cars, or any other things?\". In 2017 Google deprecated Weave and renamed Brillo to Android Things but the concept for Weaviate stayed. From the get-go, I knew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.600000000000001 / 15  (97.3):  75%|███████▌  | 15/20 [02:38<00:50, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29208, Requested 2532. Please try again in 3.48s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n",
      "Test question: How to deal with custom terminology?\n",
      "Predicted answer: To deal with custom terminology, you can use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.400000000000002 / 16  (96.3):  80%|████████  | 16/20 [02:47<00:39,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28832, Requested 2532. Please try again in 2.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {'max_tokens': 75, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 3 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29478, Requested 2532. Please try again in 4.02s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:13:33.992376Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29478, Requested 2532. Please try again in 4.02s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 15.400000000000002 / 17  (90.6):  85%|████████▌ | 17/20 [02:55<00:27,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question: How can you index data near-realtime without losing semantic meaning?\n",
      "Predicted answer: Context: [1] «This turns out to work as well! Of course, through time, the centroid calculation algorithm in Weaviate has become way more sophisticated, but the overall concept is still the same. > By validating the above two assumptions, we knew that we could almost instantly store data objects in a semantic space rather than a more traditional row-column structure or graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.6 / 18  (92.2):  90%|█████████ | 18/20 [02:56<00:13,  6.63s/it]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 2\n",
      "Overall: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 29589, Requested 2554. Please try again in 4.286s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n",
      "Test question: How do you deal with words that have multiple meanings?\n",
      "Predicted answer: Context: May contain relevant facts\n",
      "\n",
      "Question: How do you deal with words that have multiple meanings?\n",
      "\n",
      "Reasoning: Let's think step by step in order to determine how to deal with words that have multiple meanings. We need to examine the context provided to see if it mentions any specific methods or models used to address this issue. \n",
      "1. **Limitations of word2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28375, Requested 2554. Please try again in 1.858s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10f134860> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.8 / 19  (93.7):  95%|█████████▌| 19/20 [03:13<00:09,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 1\n",
      "Overall: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up request(...) after 3 tries (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28451, Requested 2554. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:dspy.evaluate.evaluate:\u001b[2m2024-09-12T19:13:53.070850Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-vhY7zZIHWbJr60TUHz326zaG on tokens per min (TPM): Limit 30000, Used 28451, Requested 2554. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m183\u001b[0m\n",
      "Average Metric: 17.8 / 20  (89.0): 100%|██████████| 20/20 [03:14<00:00,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 89.0 for set: [4]\n",
      "Scores so far: [177.0, 177.0, 88.0, 114.0, 89.0]\n",
      "Best score: 177.0\n",
      "Average of max per entry across top 1 scores: 1.77\n",
      "Average of max per entry across top 2 scores: 1.77\n",
      "Average of max per entry across top 3 scores: 1.9\n",
      "Average of max per entry across top 5 scores: 1.97\n",
      "Average of max per entry across top 8 scores: 1.97\n",
      "Average of max per entry across top 9999 scores: 1.97\n",
      "5 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric = llm_metric,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=1,\n",
    "    num_candidate_programs=2,\n",
    "    num_threads=2\n",
    ")\n",
    "\n",
    "second_compiled_rag = optimizer.compile(RAG(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Cross Encoders are used for content-based re-ranking by using a classification mechanism for data pairs to calculate a similarity score between 0 and 1, indicating the similarity between the items in the'\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: May contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n",
      "1. [Metadata Rankers](#metadata-rankers)\n",
      "1. [Score Rankers](#score-rankers)\n",
      "\n",
      "## Cross Encoders\n",
      "Cross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\n",
      "[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\n",
      "\n",
      "![Multistage search pipeline](./img/weaviate-pipeline-long.png)\n",
      "\n",
      "*Figure 5 - Multistage search pipeline using Weaviate*\n",
      "\n",
      "## Pre-trained Cross-Encoder models\n",
      "\n",
      "As noted, Cross-Encoders can achieve high *in-domain* accuracy.»\n",
      "[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\n",
      "\n",
      "Question: What do cross encoders do?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We need to gather information from the context provided about cross encoders. \n",
      "\n",
      "1. Cross Encoders are used for content-based re-ranking.\n",
      "2. They do not produce vector embeddings but use a classification mechanism for data pairs.\n",
      "3. The input to a Cross-Encoder model consists of a pair of items (e.g., two sentences) and it outputs a similarity score between 0 and 1.\n",
      "4. In search applications, Cross-Encoders are used to calculate the similarity between a search query and each data item.\n",
      "\n",
      "Answer: Cross Encoders are used for content-based re-ranking by using a classification mechanism for data pairs to calculate a similarity score between 0 and 1, indicating the similarity between the items in the\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions based on the context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: May contain relevant facts\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nContext:\\n[1] «[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\\n1. [Metadata Rankers](#metadata-rankers)\\n1. [Score Rankers](#score-rankers)\\n\\n## Cross Encoders\\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax.»\\n[2] «Bi-Encoders are fast, but are not as accurate as the expensive fisherman aka the Cross-Encoder. Cross-Encoders are time-consuming, like the fisherman who would need to limit the number of fishing rounds they could do. So we can chain those two methods behind each other (see Figure 5). First, you use a Bi-Encoder to retrieve a *list of result candidates*, then you use a Cross-Encoder on this list of candidates to pick out (or rerank) the most relevant results. This way, you benefit from the efficient retrieval method using Bi-Encoders and the high accuracy of the Cross-Encoder, so you can use this on large scale datasets!\\n\\n![Multistage search pipeline](./img/weaviate-pipeline-long.png)\\n\\n*Figure 5 - Multistage search pipeline using Weaviate*\\n\\n## Pre-trained Cross-Encoder models\\n\\nAs noted, Cross-Encoders can achieve high *in-domain* accuracy.»\\n[3] «A different strategy is using *Cross-Encoders*. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a **data pair**, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). So, individual sentences cannot be passed to a Cross-Encoder model, it always needs a **pair** of \"items\". In terms of search, you need to use the Cross-Encoder with **each** data item and the search query, to calculate the similarity between the query and data object.»\\n\\nQuestion: What do cross encoders do?\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m produce the answer. We need to gather information from the context provided about cross encoders. \\n\\n1. Cross Encoders are used for content-based re-ranking.\\n2. They do not produce vector embeddings but use a classification mechanism for data pairs.\\n3. The input to a Cross-Encoder model consists of a pair of items (e.g., two sentences) and it outputs a similarity score between 0 and 1.\\n4. In search applications, Cross-Encoders are used to calculate the similarity between a search query and each data item.\\n\\nAnswer: Cross Encoders are used for content-based re-ranking by using a classification mechanism for data pairs to calculate a similarity score between 0 and 1, indicating the similarity between the items in the\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Test question: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
      "Predicted answer: Context:\n",
      "[1] «These rewritten queries and prompts ensure that the search process better understands and retrieves relevant documents and the language model is prompted optimally. Query rewriting can be achieved as demonstrated below. ![rewrite](./img/image9.png)\n",
      "[Query Rewriting - Ma et al. 2023](https://arxiv.org/abs/2305.142\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 0.8 / 1  (80.0):  10%|█         | 1/10 [00:00<00:06,  1.41it/s]Test question: How can I retrieve the total object count in a class?\n",
      "Predicted answer: You can retrieve the total object count in a class by using the Weaviate API to perform an object count query for the specific class. This ensures that the count is accurate and specific to the class in question.\n",
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 3\n",
      "Average Metric: 3.2 / 2  (160.0):  20%|██        | 2/10 [00:01<00:05,  1.57it/s]Test question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted answer: The cosine similarity from Weaviate's certainty can be directly obtained as they are equivalent. Certainty is a number between 0 and 1, which corresponds to the cosine similarity.\n",
      "Faithful: 2\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Average Metric: 5.6 / 3  (186.7):  30%|███       | 3/10 [00:01<00:04,  1.57it/s]Test question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted answer: Context:\n",
      "[1] «|\n",
      "\n",
      "By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et al. in the self-ask paper! This result was originally placed at #6 with Hybrid Search only. This is a great opportunity to preview the discussion of how LLMs use search versus humans. When humans\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 1\n",
      "Average Metric: 6.3999999999999995 / 4  (160.0):  40%|████      | 4/10 [00:02<00:03,  1.51it/s]Test question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted answer: Context:\n",
      "[1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data, but the main interaction for data consumption goes via GraphQL.)\n",
      "\n",
      "GraphQL still follows the same constraints as REST APIs, but data is organized into a graph using only one interface.\n",
      "Faithful: 3\n",
      "Detail: 3\n",
      "Overall: 2\n",
      "Average Metric: 8.6 / 5  (172.0):  50%|█████     | 5/10 [00:03<00:03,  1.33it/s]               Test question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted answer: The best way to iterate through objects is to use the collections-first approach by creating an object for each collection and then using that object\n",
      "Faithful: 3\n",
      "Detail: 2\n",
      "Overall: 3\n",
      "Average Metric: 10.8 / 6  (180.0):  60%|██████    | 6/10 [00:04<00:03,  1.32it/s]Test question: What is best practice for updating data?\n",
      "Predicted answer: The best practice for updating data depends on the system and use case. For DSPy programs, updating can\n",
      "Faithful: 2\n",
      "Detail: 2\n",
      "Overall: 2\n",
      "Average Metric: 12.4 / 7  (177.1):  70%|███████   | 7/10 [00:04<00:02,  1.47it/s]Test question: Can I connect my own module?\n",
      "Predicted answer: Yes, you can connect your own module by uploading it to Hugging Face as a private module and then using it in Weaviate.\n",
      "Faithful: 5\n",
      "Detail: 3\n",
      "Overall: 4\n",
      "Average Metric: 15.8 / 8  (197.5):  80%|████████  | 8/10 [00:05<00:01,  1.28it/s]Test question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted answer: Context: May contain relevant facts\n",
      "\n",
      "Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "\n",
      "Reasoning: Let's think step by step in order to determine if you can train your own `text2vec-contextionary` vectorizer module. We need to look for any mention of training custom vectorizer modules, specifically the `text2vec-context\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 3\n",
      "Average Metric: 17.0 / 9  (188.9):  90%|█████████ | 9/10 [00:06<00:00,  1.39it/s]Test question: Does Weaviate use Hnswlib?\n",
      "Predicted answer: No\n",
      "Faithful: 1\n",
      "Detail: 1\n",
      "Overall: 4\n",
      "Average Metric: 18.4 / 10  (184.0): 100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e14b3 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e14b3 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e14b3_row0_col0, #T_e14b3_row0_col1, #T_e14b3_row0_col2, #T_e14b3_row1_col0, #T_e14b3_row1_col1, #T_e14b3_row1_col2, #T_e14b3_row2_col0, #T_e14b3_row2_col1, #T_e14b3_row2_col2, #T_e14b3_row3_col0, #T_e14b3_row3_col1, #T_e14b3_row3_col2, #T_e14b3_row4_col0, #T_e14b3_row4_col1, #T_e14b3_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e14b3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e14b3_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_e14b3_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_e14b3_level0_col2\" class=\"col_heading level0 col2\" >llm_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e14b3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e14b3_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_e14b3_row0_col1\" class=\"data row0 col1\" >Context: [1] «These rewritten queries and prompts ensure that the search process better understands and retrieves relevant documents and the language model is prompted optimally....</td>\n",
       "      <td id=\"T_e14b3_row0_col2\" class=\"data row0 col2\" >✔️ [0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e14b3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e14b3_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_e14b3_row1_col1\" class=\"data row1 col1\" >You can retrieve the total object count in a class by using the Weaviate API to perform an object count query for the specific class....</td>\n",
       "      <td id=\"T_e14b3_row1_col2\" class=\"data row1 col2\" >✔️ [2.4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e14b3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e14b3_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_e14b3_row2_col1\" class=\"data row2 col1\" >The cosine similarity from Weaviate's certainty can be directly obtained as they are equivalent. Certainty is a number between 0 and 1, which corresponds to...</td>\n",
       "      <td id=\"T_e14b3_row2_col2\" class=\"data row2 col2\" >✔️ [2.4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e14b3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e14b3_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_e14b3_row3_col1\" class=\"data row3 col1\" >Context: [1] «| By re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et...</td>\n",
       "      <td id=\"T_e14b3_row3_col2\" class=\"data row3 col2\" >✔️ [0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e14b3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e14b3_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_e14b3_row4_col1\" class=\"data row4 col1\" >Context: [1] «GraphQL seems the perfect solution for intuitive database interaction and efficient development. Weaviate still uses traditional RESTful endpoints (using OpenAPI/Swagger) to add data,...</td>\n",
       "      <td id=\"T_e14b3_row4_col2\" class=\"data row4 col2\" >✔️ [2.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12a11c260>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "184.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\n",
      "\u001b[93m- Prompt Model: \u001b[94m\u001b[1m10\u001b[0m\u001b[93m data summarizer calls + \u001b[94m\u001b[1m5\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program + (\u001b[94m\u001b[1m2\u001b[0m\u001b[93m) lm calls in program aware proposer = \u001b[94m\u001b[1m17\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\u001b[93m- Task Model: \u001b[94m\u001b[1m25\u001b[0m\u001b[93m examples in minibatch * \u001b[94m\u001b[1m30\u001b[0m\u001b[93m batches + \u001b[94m\u001b[1m20\u001b[0m\u001b[93m examples in train set * \u001b[94m\u001b[1m3\u001b[0m\u001b[93m full evals = \u001b[94m\u001b[1m810\u001b[0m\u001b[93m task model calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
      "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_batches`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
      "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
      "\n",
      "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False` when calling compile.\u001b[0m\n",
      "\n",
      "\u001b[93mAwaiting your input...\u001b[0m\n",
      "\n",
      "Compilation aborted by the user.\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "llm_prompter = dspy.OpenAI(model = 'gpt-4o', max_tokens = 2000, model_type='chat')\n",
    "\n",
    "optimizer = MIPROv2(\n",
    "    task_model=dspy.settings.lm,\n",
    "    metric = llm_metric,\n",
    "    prompt_model=llm_prompter,\n",
    "    num_candidates=5\n",
    ")\n",
    "\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "third_compiled_rag = optimizer.compile(RAG(), trainset=trainset, valset=devset, max_bootstrapped_demos=4, max_labeled_demos=4, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
